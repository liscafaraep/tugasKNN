{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Data Mining . . Nama : Liscafara Eldyar Pratiwi NIM : 180411100034 Mata Kuliah : Penambangan Data 5B Dosen : Mulaab S.Si. M.Kom . . Repository tugas penambangan data . . . Still learn, learn, and learn. Never give up!:)","title":"Home"},{"location":"#data-mining","text":". . Nama : Liscafara Eldyar Pratiwi NIM : 180411100034 Mata Kuliah : Penambangan Data 5B Dosen : Mulaab S.Si. M.Kom . . Repository tugas penambangan data . . . Still learn, learn, and learn. Never give up!:)","title":"Data Mining"},{"location":"tugas1/","text":"TUGAS 1 Statistika Deskriptif Mean Mean adalah nilai rata-rata dari beberapa buah data. Nilai mean dapat ditentukan dengan membagi jumlah data dengan banyaknya data. Mean (rata-rata) merupakan suatu ukuran pemusatan data. Mean suatu data juga merupakan statistik karena mampu menggambarkan bahwa data tersebut berada pada kisaran mean data tersebut. Mean tidak dapat digunakan sebagai ukuran pemusatan untuk jenis data nominal dan ordinal. Berdasarkan definisi dari mean adalah jumlah seluruh data dibagi dengan banyaknya data. Dengan kata lain jika kita memiliki N data sebagai berikut maka mean data tersebut dapat kita tuliskan sebagai berikut : Dimana: x = data ke n x bar = x rata-rata = nilai rata-rata sampel n = banyaknya data Bisa juga Menghitung mean a) Rumus Mean Hitung dari Data Tunggal b) Rumus Mean Hitung Untuk Data yang Disajikan Dalam Distribusi Frekuensi* Dengan : fixi = frekuensi untuk nilai xi yang bersesuaian xi = data ke-i c) Rumus Mean Hitung Gabungan Median Median menentukan letak tengah data setelah data disusun menurut urutan nilainya. Bisa juga nilai tengah dari data-data yang terurut . Simbol untuk median adalah Me. Dengan median Me, maka 50% dari banyak data nilainya paling tinggi sama dengan Me, dan 50% dari banyak data nilainya paling rendah sama dengan Me. Dalam mencari median, dibedakan untuk banyak data ganjil dan banyak data genap. Untuk banyak data ganjil, setelah data disusun menurut nilainya, maka median Me adalah data yang terletak tepat di tengah. Median bisa dihitung menggunakan rumus sebagai berikut: variansi merupakan salah satu ukuran sebaran yang paling sering digunakan dalam berbagai analisis statistika. Standar deviasi merupakan akar kuadrat positif dari variansi. Secara umum, variansi dirumuskun sabagai : Contoh: Dari lima kali kuiz statistika, seorang mahasiswa memperoleh nilai 82, 93, 86, 92, dan 79. Tentukan median populasi ini. jawab: Setelah data disusun dari yang terkecil sampai terbesar, diperoleh 79 82 86 92 93 Oleh karena itu medianya adalah 86 Kada nikotin yang berasal dari sebuah contoh acak enam batang rokok cap tertentu adalah 2.3, 2.7, 2.5, 2.9, 3.1, dan 1.9 miligram. Tentukan mediannya. jawab: Bila kadar nikotin itu diurutkan dari yang terkecil sampai terbesar, maka diperoleh 1.9 2.3 2.5 2.7 2.9 3.1 Maka mediannya adalah rata-rata dari 2.5 dan 2.7, yaitu Selain itu juga dapat dicari median dari data yang telah tersusun dalam bentuk distribusi frekuensi . Rumus yang digunakan ada dua, yaitu Dimana : Bak = batas kelas atas median c = lebar kelas s\u2019 = selisih antara nomor frekuensi median dengan frekuensi kumulatif sampai kelas median fM = frekuensi kelas median Sebelum menggunakan kedua rumus di atas, terlebih dahulu harus ditentukan kelas yang menjadi kelas median . Kelas median adalah kelas yang memuat nomor frekuensi median, dan nomor frekuensi median ini ditentukan dengan membagi keseluruhan data dengan dua. Modus Modus adalah nilai yang sering muncul. Jika kita tertarik pada data frekuensi, jumlah dari suatu nilai dari kumpulan data, maka kita menggunakan modus. Modus sangat baik bila digunakan untuk data yang memiliki sekala kategorik yaitu nominal atau ordinal. Sedangkan data ordinal adalah data kategorik yang bisa diurutkan, misalnya kita menanyakan kepada 100 orang tentang kebiasaan untuk mencuci kaki sebelum tidur, dengan pilihan jawaban: selalu (5), sering (4), kadang-kadang(3), jarang (2), tidak pernah (1). Apabila kita ingin melihat ukuran pemusatannya lebih baik menggunakan modus yaitu yaitu jawaban yang paling banyak dipilih, misalnya sering (2). Berarti sebagian besar orang dari 100 orang yang ditanyakan menjawab sering mencuci kaki sebelum tidur. Inilah cara menghitung modus: Data yang belum dikelompokkan Modus dari data yang belum dikelompokkan adalah ukuran yang memiliki frekuensi tertinggi. Modus dilambangkan mo. Data yang telah dikelompokkan Rumus Modus dari data yang telah dikelompokkan dihitung dengan rumus: Dengan : Mo = Modus L = Tepi bawah kelas yang memiliki frekuensi tertinggi (kelas modus) i = Interval kelas b1 = Frekuensi kelas modus dikurangi frekuensi kelas interval terdekat sebelumnya b2 = frekuensi kelas modus dikurangi frekuensi kelas interval terdekat sesudahnya Contoh: Sumbangan dari warga Bogor pada hari Palang Merah Nasional tercatat sebagai berikut: Rp 9.000, Rp 10.000, Rp 5.000, Rp 9.000, Rp 9.000, Rp 7.000, Rp 8.000, Rp 6.000, Rp 10.000, Rp 11.000. Maka modusnya, yaitu nilai yang terjadi dengan frekuensi paling tinggi, adalah Rp 9.000. Dari dua belas pelajar sekolah lanjutan tingkat atas yang diambil secara acak dicatat berapa kali mereka menonton film selama sebulan lalu. Data yang diperoleh adalah 2, 0, 3, 1, 2, 4, 2, 5, 4, 0, 1 dan 4. Dalam kasus ini terdapat dua modu, yaitu 2 dan 4, karena 2 dan 4 terdapat dengan frekuensi tertinggi. Distribusi demikian dikatakan bimodus. Standar defiasi Standar Deviasi dan Varians Salah satu teknik statistik yg digunakan untuk menjelaskan homogenitas kelompok. Varians merupakan jumlah kuadrat semua deviasi nilai-nilai individual thd rata-rata kelompok. Sedangkan akar dari varians disebut dengan standar deviasi atau simpangan baku. Standar Deviasi dan Varians Simpangan baku merupakan variasi sebaran data. Semakin kecil nilai sebarannya berarti variasi nilai data makin sama Jika sebarannya bernilai 0, maka nilai semua datanya adalah sama. Semakin besar nilai sebarannya berarti data semakin bervariasi. Cara penulisan rumus fungsi standar deviasi STDEV (number1, number2,\u2026) Dengan : Number1, number2, \u2026 adalah 1-255 argumen yang sesuai dengan sampel populasi. Anda juga dapat menggunakan array tunggal atau referensi ke array, bukan argumen yang dipisahkan oleh koma. Keterangan a. STDEV mengasumsikan bahwa argumen adalah contoh dari populasi. Jika data anda mewakili seluruh populasi, untuk menghitung deviasi standar menggunakan STDEVP. b. Standar deviasi dihitung menggunakan metode \u201cn-1\u2033 . c. Argumen dapat berupa nomor atau nama, array, atau referensi yang mengandung angka. d. Nilai-nilai logis dan representasi teks dari nomor yang Anda ketik langsung ke daftar argumen akan dihitung. e. Jika argumen adalah sebuah array atau referensi, hanya nomor/angka dalam array atau referensi yang akan dihitung. Sel kosong, nilai-nilai logis, teks, atau nilai-nilai kesalahan dalam array atau referensi akan diabaikan. f. Argumen yang kesalahan nilai atau teks yang tidak dapat diterjemahkan ke dalam nomor/angka akan menyebabkan kesalahan. g. Jika Anda ingin memasukkan nilai-nilai logis dan representasi teks angka dalam referensi sebagai bagian dari perhitungan, gunakan fungsi STDEVA. Dalam penerapannya STDEV , perhitungan standar deviasi secara manual menggunakan rumus berikut: Dimana: x = data ke n x bar = x rata-rata = nilai rata-rata sampel n = banyaknya data variansi merupakan salah satu ukuran sebaran yang paling sering digunakan dalam berbagai analisis statistika. Standar deviasi merupakan akar kuadrat positif dari variansi. Secara umum, variansi dirumuskun sabagai : Jika kita memiliki n observasi yaitu X1,X2,\u2026.Xn, dan diketahui Xbar adalah rata-rata sampel yang dimiliki, maka variansi dapat dihitung sebagai : Contoh: Jika dimiliki data : 210, 340, 525, 450, 275 maka variansi dan standar deviasinya : mean = (210, 340, 525, 450, 275)/5 = 360 variansi dan standar deviasi berturut-turut : Sedangkan jika data disajikan dalam tabel distribusi frekuensi, variansi sampel dapat dihitung sebagai : Jangkauan (Range) Yang dimaksud jangkauan atau range adalah selisih data terbesar dengan data terkecil dari sekumpulan data. Jangkauan biasanya diberi simbol R. Cara menghitung jangkauan adalah dengan menggunakan rumus menghitung jangkauan dari sejumlah data berikut ini. jangkauan = data terbesar \u2013 data terkecil Misalkan kita akan menghitung jangkauan dari 8 data berikut: 15, 16, 17, 21, 14, 19, 20, 15. Berdasarkan data tersebut data terbesar adalah 21, dan data terkecil adalah 14 Jangkauan = data terbesar \u2013 data terkecil = 21 \u2013 14 = 7 Kuartil Yang dimaksud dengan kuartil adalah data yang membagi posisi sekumpulan data yang telah diurutkan menjadi empat bagian. Dalam satu urutan data terdapat 3 kuartil yaitu kuartil bawah, kuartil tengah, dan kuartil atas. Cara menentukan kuartil adalah sebagai berikut. Kuartil bawah adalah data pada posisi 1/4 dari kumpulan data yang telah diurutkan. Kuartil bawah disimbolkan dengan Q1. Kuartil tengah adalah data pada posisi 2/4 dari kumpulan data yang telah diurutkan. Kuartil tengah sama dengan median. Kuartil tengah disimbolkan dengan Q2. Kuartil atas adalah data pada posisi 3/4 dari kumpulan data yang telah diurutkan. Kuartil atas disimbolkan dengan Q3. Posisi ketiga kuartil ditentukan dari rumus berikut. Posisi Qi = i(n+1)/4 i = indeks kuartil yaitu 1, 2, 3 dan n = banyaknya data Misalkan kita akan menentukan kuartil bawah, tengah, dan kuartil atas dari 15 data berikut: 11, 24, 12, 15, 12, 18, 22, 25, 26, 27, 17, 22, 24, 19, 12. Urutan data dari yang terkecil: 11, 12, 12, 12, 15, 17, 18, 19, 22, 22, 24, 24, 25, 26, 27 Posisi ketiga kuartil adalah sebagai berikut Posisi Q1 = 1.(15+1)/4 = (16)/4 = 4 (data urutan ke 4) Posisi Q2 = 2. (15+1)/4 = 2(16)/4 = 8 (data urutan ke 4) Posisi Q3 = 3. (15+1)/4 = 3(16)/4 = 12 (data urutan ke 4) Berdasarkan posisi kuartil pada urutan data maka dapat ditentukan ketiga kuartil 11, 12, 12, 12 , 15, 17, 18, 19 , 22, 22, 24, 24 , 25, 26, 27 Jadi kuartil bawah adalah 12 Kuartil tengah = median = 19 Kuartil atas = 24 Jangkauan Antar Kuartil ( Hamparan) Yang dimaksud jangkauan antar kuartil atau hamparan adalah selisih antara kuartil atas dengan kuartil bawah. Jangkauan antar kuartil diberi simbol H. H = Q3 \u2013 Q1 Jangkauan Semi Antar Kuartil (Simpangan Kuartil) Yang dimaksud semi antar kuartil atau simpangan kuartil adalah setengah dari selisih antara kuartil atas dengan kuartil bawah. Jangkauan semi antar kuartil diberi simbol Qd. Qd = \u00bd ( Q3 \u2013 Q1) Desil Yang dimaksud dengan desil adalah data yang membagi posisi sekumpulan data yang telah diurutkan menjadi sepuluh bagian. Dalam satu urutan data terdapat 9 desil, masing masing disebut D1 sampai D9. Persentil Yang dimaksud dengan persentil adalah data yang membagi posisi sekumpulan data yang telah diurutkan menjadi seratus bagian. Dalam satu urutan data terdapat 99 persentil, masing masing disebut P1 sampai P99. Jangkauan persentil dirumuskan sebagai berikut. Jangkauan persentil = P90 \u2013 P10 Contoh Program Coba run program di bawah ini menggunakan Jupyter Notebook dan lihat hasilnya! import pandas as pd df = pd.read_csv('daftarnilai.csv') df Hasil run program : Matematika Fisika Kimia Biologi 0 67 77 71 77 1 72 83 70 79 2 75 92 80 75 3 65 79 77 90 4 66 72 67 92 ... ... ... ... ... 95 73 89 70 87 96 85 83 85 93 97 71 86 84 84 98 77 78 77 78 99 81 82 83 84 100 rows \u00d7 4 columns Contoh program menentukan mean, nilai maks, nilai min, kuartil, standar deviasi dan lain-lain pada banyak data. from IPython.display import HTML, display import tabulate table=[ [\"method\"]+[x for x in df.columns], [\"describe()\"]+['<pre>'+str(df[col].describe())+'</pre>' for col in df.columns], [\"count()\"]+[df[col].count() for col in df.columns], [\"mean()\"]+[df[col].mean() for col in df.columns], [\"std()\"]+[\"{:.2f}\".format(df[col].std()) for col in df.columns], [\"min()\"]+[df[col].min() for col in df.columns], [\"max()\"]+[df[col].max() for col in df.columns], [\"q1()\"]+[df[col].quantile(0.25) for col in df.columns], [\"q2()\"]+[df[col].quantile(0.50) for col in df.columns], [\"q3()\"]+[df[col].quantile(0.75) for col in df.columns], [\"skew()\"]+[\"{:.2f}\".format(df[col].skew()) for col in df.columns], ] display(HTML(tabulate.tabulate(table, talefmt='html'))) Hasil run program : Contoh program menentukan kurva pada sebuah tabel data. import pandas as pd import seaborn as sns df = pd.read_csv(\"daftarnilai.csv\",usecols=[0]) sns.distplot(df) <matplotlib.axes._subplots.AxesSubplot at 0xadb4ed0> Untuk file lengkapnya dapat dicek di sini Sumber : http://blog.ub.ac.id/adiarsa/2012/03/14/mean-median-modus-dan-standar-deviasi/ http://ukurandansatuan.com/cara-menghitung-mean-median-modus-jangkauan-kuartil-desil-dan-persentil.html/ \u200b","title":"Tugas 1"},{"location":"tugas1/#tugas-1","text":"","title":"TUGAS 1"},{"location":"tugas1/#statistika-deskriptif","text":"Mean Mean adalah nilai rata-rata dari beberapa buah data. Nilai mean dapat ditentukan dengan membagi jumlah data dengan banyaknya data. Mean (rata-rata) merupakan suatu ukuran pemusatan data. Mean suatu data juga merupakan statistik karena mampu menggambarkan bahwa data tersebut berada pada kisaran mean data tersebut. Mean tidak dapat digunakan sebagai ukuran pemusatan untuk jenis data nominal dan ordinal. Berdasarkan definisi dari mean adalah jumlah seluruh data dibagi dengan banyaknya data. Dengan kata lain jika kita memiliki N data sebagai berikut maka mean data tersebut dapat kita tuliskan sebagai berikut : Dimana: x = data ke n x bar = x rata-rata = nilai rata-rata sampel n = banyaknya data Bisa juga Menghitung mean a) Rumus Mean Hitung dari Data Tunggal b) Rumus Mean Hitung Untuk Data yang Disajikan Dalam Distribusi Frekuensi* Dengan : fixi = frekuensi untuk nilai xi yang bersesuaian xi = data ke-i c) Rumus Mean Hitung Gabungan Median Median menentukan letak tengah data setelah data disusun menurut urutan nilainya. Bisa juga nilai tengah dari data-data yang terurut . Simbol untuk median adalah Me. Dengan median Me, maka 50% dari banyak data nilainya paling tinggi sama dengan Me, dan 50% dari banyak data nilainya paling rendah sama dengan Me. Dalam mencari median, dibedakan untuk banyak data ganjil dan banyak data genap. Untuk banyak data ganjil, setelah data disusun menurut nilainya, maka median Me adalah data yang terletak tepat di tengah. Median bisa dihitung menggunakan rumus sebagai berikut: variansi merupakan salah satu ukuran sebaran yang paling sering digunakan dalam berbagai analisis statistika. Standar deviasi merupakan akar kuadrat positif dari variansi. Secara umum, variansi dirumuskun sabagai : Contoh: Dari lima kali kuiz statistika, seorang mahasiswa memperoleh nilai 82, 93, 86, 92, dan 79. Tentukan median populasi ini. jawab: Setelah data disusun dari yang terkecil sampai terbesar, diperoleh 79 82 86 92 93 Oleh karena itu medianya adalah 86 Kada nikotin yang berasal dari sebuah contoh acak enam batang rokok cap tertentu adalah 2.3, 2.7, 2.5, 2.9, 3.1, dan 1.9 miligram. Tentukan mediannya. jawab: Bila kadar nikotin itu diurutkan dari yang terkecil sampai terbesar, maka diperoleh 1.9 2.3 2.5 2.7 2.9 3.1 Maka mediannya adalah rata-rata dari 2.5 dan 2.7, yaitu Selain itu juga dapat dicari median dari data yang telah tersusun dalam bentuk distribusi frekuensi . Rumus yang digunakan ada dua, yaitu Dimana : Bak = batas kelas atas median c = lebar kelas s\u2019 = selisih antara nomor frekuensi median dengan frekuensi kumulatif sampai kelas median fM = frekuensi kelas median Sebelum menggunakan kedua rumus di atas, terlebih dahulu harus ditentukan kelas yang menjadi kelas median . Kelas median adalah kelas yang memuat nomor frekuensi median, dan nomor frekuensi median ini ditentukan dengan membagi keseluruhan data dengan dua. Modus Modus adalah nilai yang sering muncul. Jika kita tertarik pada data frekuensi, jumlah dari suatu nilai dari kumpulan data, maka kita menggunakan modus. Modus sangat baik bila digunakan untuk data yang memiliki sekala kategorik yaitu nominal atau ordinal. Sedangkan data ordinal adalah data kategorik yang bisa diurutkan, misalnya kita menanyakan kepada 100 orang tentang kebiasaan untuk mencuci kaki sebelum tidur, dengan pilihan jawaban: selalu (5), sering (4), kadang-kadang(3), jarang (2), tidak pernah (1). Apabila kita ingin melihat ukuran pemusatannya lebih baik menggunakan modus yaitu yaitu jawaban yang paling banyak dipilih, misalnya sering (2). Berarti sebagian besar orang dari 100 orang yang ditanyakan menjawab sering mencuci kaki sebelum tidur. Inilah cara menghitung modus: Data yang belum dikelompokkan Modus dari data yang belum dikelompokkan adalah ukuran yang memiliki frekuensi tertinggi. Modus dilambangkan mo. Data yang telah dikelompokkan Rumus Modus dari data yang telah dikelompokkan dihitung dengan rumus: Dengan : Mo = Modus L = Tepi bawah kelas yang memiliki frekuensi tertinggi (kelas modus) i = Interval kelas b1 = Frekuensi kelas modus dikurangi frekuensi kelas interval terdekat sebelumnya b2 = frekuensi kelas modus dikurangi frekuensi kelas interval terdekat sesudahnya Contoh: Sumbangan dari warga Bogor pada hari Palang Merah Nasional tercatat sebagai berikut: Rp 9.000, Rp 10.000, Rp 5.000, Rp 9.000, Rp 9.000, Rp 7.000, Rp 8.000, Rp 6.000, Rp 10.000, Rp 11.000. Maka modusnya, yaitu nilai yang terjadi dengan frekuensi paling tinggi, adalah Rp 9.000. Dari dua belas pelajar sekolah lanjutan tingkat atas yang diambil secara acak dicatat berapa kali mereka menonton film selama sebulan lalu. Data yang diperoleh adalah 2, 0, 3, 1, 2, 4, 2, 5, 4, 0, 1 dan 4. Dalam kasus ini terdapat dua modu, yaitu 2 dan 4, karena 2 dan 4 terdapat dengan frekuensi tertinggi. Distribusi demikian dikatakan bimodus. Standar defiasi Standar Deviasi dan Varians Salah satu teknik statistik yg digunakan untuk menjelaskan homogenitas kelompok. Varians merupakan jumlah kuadrat semua deviasi nilai-nilai individual thd rata-rata kelompok. Sedangkan akar dari varians disebut dengan standar deviasi atau simpangan baku. Standar Deviasi dan Varians Simpangan baku merupakan variasi sebaran data. Semakin kecil nilai sebarannya berarti variasi nilai data makin sama Jika sebarannya bernilai 0, maka nilai semua datanya adalah sama. Semakin besar nilai sebarannya berarti data semakin bervariasi. Cara penulisan rumus fungsi standar deviasi STDEV (number1, number2,\u2026) Dengan : Number1, number2, \u2026 adalah 1-255 argumen yang sesuai dengan sampel populasi. Anda juga dapat menggunakan array tunggal atau referensi ke array, bukan argumen yang dipisahkan oleh koma. Keterangan a. STDEV mengasumsikan bahwa argumen adalah contoh dari populasi. Jika data anda mewakili seluruh populasi, untuk menghitung deviasi standar menggunakan STDEVP. b. Standar deviasi dihitung menggunakan metode \u201cn-1\u2033 . c. Argumen dapat berupa nomor atau nama, array, atau referensi yang mengandung angka. d. Nilai-nilai logis dan representasi teks dari nomor yang Anda ketik langsung ke daftar argumen akan dihitung. e. Jika argumen adalah sebuah array atau referensi, hanya nomor/angka dalam array atau referensi yang akan dihitung. Sel kosong, nilai-nilai logis, teks, atau nilai-nilai kesalahan dalam array atau referensi akan diabaikan. f. Argumen yang kesalahan nilai atau teks yang tidak dapat diterjemahkan ke dalam nomor/angka akan menyebabkan kesalahan. g. Jika Anda ingin memasukkan nilai-nilai logis dan representasi teks angka dalam referensi sebagai bagian dari perhitungan, gunakan fungsi STDEVA. Dalam penerapannya STDEV , perhitungan standar deviasi secara manual menggunakan rumus berikut: Dimana: x = data ke n x bar = x rata-rata = nilai rata-rata sampel n = banyaknya data variansi merupakan salah satu ukuran sebaran yang paling sering digunakan dalam berbagai analisis statistika. Standar deviasi merupakan akar kuadrat positif dari variansi. Secara umum, variansi dirumuskun sabagai : Jika kita memiliki n observasi yaitu X1,X2,\u2026.Xn, dan diketahui Xbar adalah rata-rata sampel yang dimiliki, maka variansi dapat dihitung sebagai : Contoh: Jika dimiliki data : 210, 340, 525, 450, 275 maka variansi dan standar deviasinya : mean = (210, 340, 525, 450, 275)/5 = 360 variansi dan standar deviasi berturut-turut : Sedangkan jika data disajikan dalam tabel distribusi frekuensi, variansi sampel dapat dihitung sebagai : Jangkauan (Range) Yang dimaksud jangkauan atau range adalah selisih data terbesar dengan data terkecil dari sekumpulan data. Jangkauan biasanya diberi simbol R. Cara menghitung jangkauan adalah dengan menggunakan rumus menghitung jangkauan dari sejumlah data berikut ini. jangkauan = data terbesar \u2013 data terkecil Misalkan kita akan menghitung jangkauan dari 8 data berikut: 15, 16, 17, 21, 14, 19, 20, 15. Berdasarkan data tersebut data terbesar adalah 21, dan data terkecil adalah 14 Jangkauan = data terbesar \u2013 data terkecil = 21 \u2013 14 = 7 Kuartil Yang dimaksud dengan kuartil adalah data yang membagi posisi sekumpulan data yang telah diurutkan menjadi empat bagian. Dalam satu urutan data terdapat 3 kuartil yaitu kuartil bawah, kuartil tengah, dan kuartil atas. Cara menentukan kuartil adalah sebagai berikut. Kuartil bawah adalah data pada posisi 1/4 dari kumpulan data yang telah diurutkan. Kuartil bawah disimbolkan dengan Q1. Kuartil tengah adalah data pada posisi 2/4 dari kumpulan data yang telah diurutkan. Kuartil tengah sama dengan median. Kuartil tengah disimbolkan dengan Q2. Kuartil atas adalah data pada posisi 3/4 dari kumpulan data yang telah diurutkan. Kuartil atas disimbolkan dengan Q3. Posisi ketiga kuartil ditentukan dari rumus berikut. Posisi Qi = i(n+1)/4 i = indeks kuartil yaitu 1, 2, 3 dan n = banyaknya data Misalkan kita akan menentukan kuartil bawah, tengah, dan kuartil atas dari 15 data berikut: 11, 24, 12, 15, 12, 18, 22, 25, 26, 27, 17, 22, 24, 19, 12. Urutan data dari yang terkecil: 11, 12, 12, 12, 15, 17, 18, 19, 22, 22, 24, 24, 25, 26, 27 Posisi ketiga kuartil adalah sebagai berikut Posisi Q1 = 1.(15+1)/4 = (16)/4 = 4 (data urutan ke 4) Posisi Q2 = 2. (15+1)/4 = 2(16)/4 = 8 (data urutan ke 4) Posisi Q3 = 3. (15+1)/4 = 3(16)/4 = 12 (data urutan ke 4) Berdasarkan posisi kuartil pada urutan data maka dapat ditentukan ketiga kuartil 11, 12, 12, 12 , 15, 17, 18, 19 , 22, 22, 24, 24 , 25, 26, 27 Jadi kuartil bawah adalah 12 Kuartil tengah = median = 19 Kuartil atas = 24 Jangkauan Antar Kuartil ( Hamparan) Yang dimaksud jangkauan antar kuartil atau hamparan adalah selisih antara kuartil atas dengan kuartil bawah. Jangkauan antar kuartil diberi simbol H. H = Q3 \u2013 Q1 Jangkauan Semi Antar Kuartil (Simpangan Kuartil) Yang dimaksud semi antar kuartil atau simpangan kuartil adalah setengah dari selisih antara kuartil atas dengan kuartil bawah. Jangkauan semi antar kuartil diberi simbol Qd. Qd = \u00bd ( Q3 \u2013 Q1) Desil Yang dimaksud dengan desil adalah data yang membagi posisi sekumpulan data yang telah diurutkan menjadi sepuluh bagian. Dalam satu urutan data terdapat 9 desil, masing masing disebut D1 sampai D9. Persentil Yang dimaksud dengan persentil adalah data yang membagi posisi sekumpulan data yang telah diurutkan menjadi seratus bagian. Dalam satu urutan data terdapat 99 persentil, masing masing disebut P1 sampai P99. Jangkauan persentil dirumuskan sebagai berikut. Jangkauan persentil = P90 \u2013 P10","title":"Statistika Deskriptif"},{"location":"tugas1/#contoh-program","text":"Coba run program di bawah ini menggunakan Jupyter Notebook dan lihat hasilnya! import pandas as pd df = pd.read_csv('daftarnilai.csv') df Hasil run program : Matematika Fisika Kimia Biologi 0 67 77 71 77 1 72 83 70 79 2 75 92 80 75 3 65 79 77 90 4 66 72 67 92 ... ... ... ... ... 95 73 89 70 87 96 85 83 85 93 97 71 86 84 84 98 77 78 77 78 99 81 82 83 84 100 rows \u00d7 4 columns Contoh program menentukan mean, nilai maks, nilai min, kuartil, standar deviasi dan lain-lain pada banyak data. from IPython.display import HTML, display import tabulate table=[ [\"method\"]+[x for x in df.columns], [\"describe()\"]+['<pre>'+str(df[col].describe())+'</pre>' for col in df.columns], [\"count()\"]+[df[col].count() for col in df.columns], [\"mean()\"]+[df[col].mean() for col in df.columns], [\"std()\"]+[\"{:.2f}\".format(df[col].std()) for col in df.columns], [\"min()\"]+[df[col].min() for col in df.columns], [\"max()\"]+[df[col].max() for col in df.columns], [\"q1()\"]+[df[col].quantile(0.25) for col in df.columns], [\"q2()\"]+[df[col].quantile(0.50) for col in df.columns], [\"q3()\"]+[df[col].quantile(0.75) for col in df.columns], [\"skew()\"]+[\"{:.2f}\".format(df[col].skew()) for col in df.columns], ] display(HTML(tabulate.tabulate(table, talefmt='html'))) Hasil run program : Contoh program menentukan kurva pada sebuah tabel data. import pandas as pd import seaborn as sns df = pd.read_csv(\"daftarnilai.csv\",usecols=[0]) sns.distplot(df) <matplotlib.axes._subplots.AxesSubplot at 0xadb4ed0> Untuk file lengkapnya dapat dicek di sini Sumber : http://blog.ub.ac.id/adiarsa/2012/03/14/mean-median-modus-dan-standar-deviasi/ http://ukurandansatuan.com/cara-menghitung-mean-median-modus-jangkauan-kuartil-desil-dan-persentil.html/ \u200b","title":"Contoh Program"},{"location":"tugas2/","text":"Tugas 2 Mengukur Jarak Data Mengukur Jarak Tipe Numerik Salah satu tantangan dalam era ini dengan datatabase yang memiliki banyak tipe data. Mengukur jarak adalah komponen utama dalam algoritma clustering berbasis jarak. Alogritma seperit Algoritma Partisioning misal K-Mean, K-medoidm dan fuzzy c-mean dan rough clustering bergantung pada jarak untuk melakukan pengelompokkan Sebelum menjelaskan tentang beberapa macam ukuran jarak, kita mendefinisikan terlebih dahulu yaiut v1,v2v1,v2 menyatakan dua vektor yang menyatakan v1=x1,x2,...,xn,v2=y1,y2,...,yn,v1=x1,x2,...,xn,v2=y1,y2,...,yn, dimana xi,yixi,yi disebut attribut. Ada beberapa ukuran similaritas datau ukuran jarak, diantaranya : Minkowski Distance Kelompok Minkowski diantaranya adalah Euclidean distance dan Manhattan distance, yang menjadi kasus khusus dari Minkowski distance. Minkowski distance dinyatakan dengan dimana m adalah bilangan riel positif dan x i dan y i adalah dua vektor dalam runang dimensi nn Implementasi ukuran jarak Minkowski pada model clustering data atribut dilakukan normalisasi untuk menghindari dominasi dari atribut yang memiliki skala data besar. Manhattan distance Manhattan distance adalah kasus khsusu dari jarak Minkowski distance pada m = 1. Seperti Minkowski Distance, Manhattan distance sensitif terhadap outlier. BIla ukuran ini digunakan dalam algoritma clustering , bentuk cluster adalah hyper-rectangular. Ukuran ini didefinisikan dengan Euclidean distance Jarak yang paling terkenal yang digunakan untuk data numerik adalah jarak Euclidean. Ini adalah kasus khusus dari jarak Minkowski ketika m = 2. Jarak Euclidean berkinerja baik ketika digunakan untuk kumpulan data cluster kompak atau terisolasi . Meskipun jarak Euclidean sangat umum dalam pengelompokan, ia memiliki kelemahan: jika dua vektor data tidak memiliki nilai atribut yang sama, kemungkin memiliki jarak yang lebih kecil daripada pasangan vektor data lainnya yang mengandung nilai atribut yang sama. Masalah lain dengan jarak Euclidean sebagai fitur skala terbesar akan mendominasi yang lain. Normalisasi fitur kontinu adalah solusi untuk mengatasi kelemahan ini. Average Distance Berkenaan dengan kekurangan dari Jarak Euclidian Distance diatas, rata rata jarak adala versi modikfikasid ari jarak Euclidian untuk memperbaiki hasil. Untuk dua titik x,yx,y dalam ruang dimensi nn, rata-rata jarak didefinisikan dengan Weighted euclidean distance Jika berdasarkan tingkatan penting dari masing masing atribut ditentukan, maka Weighted Euclidean distance adalah modifikisasi lain dari jarak Euclidean distance yang dapat digunakan. Ukuran ini dirumuskan dengan dimana wi adalah bobot yang diberikan pada atribut ke i. Chord distance Chord distance adalah salah satu ukuran jarak modifikasi Euclidean distance untuk mengatasi kekurangan dari Euclidean distance. Ini dapat dipecahkan juga dengan menggunakan skala pengukuran yang baik. Jarak ini dapat juga dihitung dari data yang tidak dinormalisasi . Chord distance didefinisikan dengan dimana \u2016 x \u20162 adalah L 2-norm . Mahalanobis distance Mahalanobis distance berdasarkan data berbeda dengan Euclidean dan Manhattan distances yang bebas antra data dengan data yang lain. Jarak Mahalanobis yang teratur dapat digunakan untuk mengekstraksi hyperellipsoidal clusters. Jarak Mahalanobis dapat mengurangi distorsi yang disebabkan oleh korelasi linier antara fitur dengan menerapkan transformasi pemutihan ke data atau dengan menggunakan kuadrat Jarak mahalanobis. Mahalanobis distance dinyatakan dengan dimana S adalah matrik covariance data. Cosine measure Ukuran Cosine similarity lebih banyak digunakan dalam similaritas dokumen dan dinyatakan dengan dimana \u2225y\u22252 adalah Euclidean norm dari vektor y=(y1,y2,\u2026,yn)y=(y1,y2,\u2026,yn) didefinisikan dengan Pearson correlation Pearson correlation banyak digunakan dalam data expresi gen. Ukuran similaritas ini menghitung similaritas antara duan bentuk pola expresi gen. Pearson correlation didefinisikan dengan , where \u03bc x The Pearson correlation kelemahannya adalah sensitif terhadap outlier Mengukur Jarak Atribut Binary Mari kita lihat similaritas dan desimilirity untuk objek yang dijelaskan oleh atribut biner simetris atau asimetris. Aatribut biner hanya memiliki dua status: 0 dan 1 Contoh atribut perokok menggambarkan seorang pasien, misalnya, 1 menunjukkan bahwa pasien merokok, sedangkan 0 menunjukkan pasien tidak merokok. Memperlakukan atribut biner sebagai atribut numerik tidak diperkenankan. Oleh karena itu, metode khusus untuk data biner diperlukan untuk membedakan komputasi. Jadi, bagaimana kita bisa menghitung ketidaksamaan antara dua atribut biner? \u201dSatu pendekatan melibatkan penghitungan matriks ketidaksamaan dari data biner yang diberikan. Jika semua atribut biner dianggap memiliki bobot yang sama, kita memiliki tabel kontingensi 2\u00d72 di mana qq adalah jumlah atribut yang sama dengan 1 untuk kedua objek ii dan jj, rr adalah jumlah atribut yang sama dengan 1 untuk objek ii tetapi 0 untuk objek jj, ss adalah jumlah atribut yang sama dengan 0 untuk objek ii tetapi 1 untuk objek jj, dan tt adalah jumlah atribut yang sama dengan 0 untuk kedua objek ii dan jj. Jumlah total atribut adalah pp, di mana p=q+r+s+tp=q+r+s+t Ingatlah bahwa untuk atribut biner simetris, masing-masing nilai bobot yang sama.Dissimilarity yang didasarkan pada atribut aymmetric binary disebut symmetric binary dissimilarity. Jika objek i dan j dinyatakan sebagai atribut biner simetris, maka dissimilarity antarii dan j adalah $$ d ( i , j ) = \\frac { r + s } { q + r + s + t } $$ Untuk atribut biner asimetris, kedua kondisi tersebut tidak sama pentingnya, seperti hasil positif (1) dan negatif (0) dari tes penyakit. Diberikan dua atribut biner asimetris, pencocokan keduanya 1 (kecocokan positif) kemudian dianggap lebih signifikan daripada kecocokan negatif. Ketidaksamaan berdasarkan atribut-atribut ini disebut asimetris biner dissimilarity, di mana jumlah kecocokan negatif, t, dianggap tidak penting dan dengan demikian diabaikan. Berikut perhitungannya $$ d ( i , j ) = \\frac { r + s } { q + r + s } $$ Kita dapat mengukur perbedaan antara dua atribut biner berdasarkan pada disimilarity. Misalnya, biner asimetris kesamaan antara objek ii dan jj dapat dihitung dengan $$ \\operatorname { sim } ( i , j ) = \\frac { q } { q + r + s } = 1 - d ( i , j ) $$ Persamaan similarity ini disebut dengan Jaccard coefficient Mengukur Jarak Tipe categorical Li, C., & Li, H. (2010). A Survey of Distance Metrics for Nominal Attributes. JSW, 5(11), 1262-1269. Overlay Metric Ketika semua atribut adalah bertipe nominal, ukuran jarak yang paling sederhana adalah dengan Ovelay Metric (OM) yang dinyatakan dengan $$ d ( x , y ) = \\sum _ { i = 1 } ^ { n } \\delta ( a _ { i } ( x ) , a _ { i } ( y ) ) $$ dimana nn adalah banyaknya atribut, ai(x)ai(x) dan ai(y)ai(y) adalah nilai atribut ke ii yaitu AiAi dari masing masing objek xx dan yy, \u03b4 (ai(x),ai(y))\u03b4 (ai(x),ai(y)) adalah 0 jika ai(x)=ai(y) dan 1 jika sebaliknya. OM banyak digunakan oleh instance-based learning dan locally weighted learning. Jelas sekali , ini sedikit beruk untuk mengukur jarak antara masing-masing pasangan sample, karena gagal memanfaatkan tambahan informasi yang diberikan oleh nilai atribut nominal yang bisa membantu dalam generalisasi. Value Difference Metric (VDM) VDM dikenalkan oleh Standfill and Waltz, versi sederhana dari VDM tanpa skema pembobotan didefinsisikan dengan $$ d ( x , y ) = \\sum _ { i = 1 } ^ { n } \\sum _ { c = 1 } ^ { C } \\left| P ( c | a _ { i } ( x ) ) - P ( c | a _ { i } ( y ) ) \\right | $$ dimana CCadalah banyaknya kelas, P(c|ai(x))P(c|ai(x)) adalah probabilitas bersyarat dimana kelas xx adalah cc dari atribut AiAi, yang memiliki nilai ai(x)ai(x), P(c|ai(y))P(c|ai(y)) adalah probabilitas bersyarat dimana kelas yy adalah cc dengan atribut AiAi memiliki nilai ai(y)ai(y) VDM mengasumsikan bahwa dua nilai dari atribut adalah lebih dekat jika memiliki klasifikasi sama. Pendekatan lain berbasi probabilitas adalah SFM (Short and Fukunaga Metric) yang kemudian dikembangkan oleh Myles dan Hand dan didefinisikan dengan $$ d ( x , y ) = \\sum _ { c = 1 } ^ { C } \\left | P ( c | x ) - P ( c | y ) \\right| $$ diman probabilitas keanggotaan kelas diestimasi dengan P(c|x) dan P(c|y) didekati dengan Naive Bayes, Minimum Risk Metric (MRM) Ukuran ini dipresentasikan oleh Blanzieri and Ricci, berbeda dari SFM yaitu meminimumkan selisih antara kesalahan berhingga dan kesalahan asymtotic. MRM meminimumkan risk of misclassification yang didefinisikan dengan $$ d ( x , y ) = \\sum _ { c = 1 } ^ { C } P ( c | x ) ( 1 - P ( c | y ) ) $$ Mengukur Jarak Tipe Ordinal Han, J., Pei, J., & Kamber, M. (2011). Data mining: concepts and techniques. Elsevier . Nilai-nilai atribut ordinal memiliki urutan atau peringkat, namun besarnya antara nilai-nilai berturut-turut tidak diketahui. Contohnya tingkatan kecil, sedang, besar untuk atribut ukuran. Atribut ordinal juga dapat diperoleh dari diskritisasi atribut numerik dengan membuat rentang nilai ke dalam sejumlah kategori tertentu. Kategori-kategori ini disusun dalam peringkat. Yaitu, rentang atribut numerik dapat dipetakan ke atribut ordinal ff yang memiliki MfMf state. Misalnya, kisaran suhu atribut skala-skala (dalam Celcius)dapat diatur ke dalam status berikut: \u221230 hingga \u221210, \u221210 hingga 10, 10 hingga 30, masing-masing mewakili kategori suhu dingin, suhu sedang, dan suhu hangat. MM adalah jumlah keadaan yang dapat dilakukan oleh atribut ordinalmemiliki. State ini menentukan peringkat 1,...,Mf1,...,Mf Perlakuan untuk atribut ordinal adalah cukup sama dengan atribut numerik ketika menghitung disimilarity antara objek. Misalkan ff adalah atribut-atribut dari atribut ordinal dari nn objek. Menghitung disimilarity terhadap f fitur sebagai berikut: Nilai ff untuk objek ke-ii adalah xifxif, dan ff memiliki MfMf status urutan , mewakili peringkat 1,..,Mf1,..,Mf Ganti setiap xifxif dengan peringkatnya, rif\u2208{1...Mf}rif\u2208{1...Mf} Karena setiap atribut ordinal dapat memiliki jumlah state yang berbeda, diperlukan untuk memetakan rentang setiap atribut ke [0,0, 1.0] sehingga setiap atribut memiliki bobot yang sama. Perl melakukan normalisasi data dengan mengganti peringkat rifrif dengan $$ z _ { i f } = \\frac { r _ { i f } - 1 } { M _ { f } - 1 } $$ Dissimilarity kemudian dihitung dengan menggunakan ukuran jarak seperti atribut numerik dengan data yang baru setelah ditransformasi $ z _ { i f }$ Menghitung Jarak Tipe Campuran Wilson, D. R., & Martinez, T. R. (1997). Improved heterogeneous distance functions. Journal of artificial intelligence research, 6, 1-34. Menghitung ketidaksamaan antara objek dengan atribut campuran yang berupa nominal, biner simetris, biner asimetris, numerik, atau ordinal yang ada pada kebanyakan databasae dapat dinyatakan dengan memproses semua tipe atribut secara bersamaan. Salah satu teknik tersebut menggabungkan atribut yang berbeda ke dalam matriks ketidaksamaan tunggal dan menyatakannya dengan skala interval antar [0,0,1.0][0,0,1.0]. Misalkan data berisi atribut pp tipe campuran. Ketidaksamaan (disimilarity ) antara objek ii dan jj dinyatakan dengan $$ d ( i , j ) = \\frac { \\sum _ { f = 1 } ^ { p } \\delta _ { i j } ^ { ( f ) } d _ { i j } ^ { ( f ) } } { \\sum _ { f = 1 } ^ { p } \\delta _ { i j } ^ { ( f ) } } $$ dimana \u03b4fij=0\u03b4ijf=0 - jika xifxif atau xjfxjf adalah hilang (i.e., tidak ada pengukuran dari atribut f untuk objek ii atau objek jj) jika xif=xjf=0xif=xjf=0 dan atribut ff adalah binary asymmetric, selain itu \u03b4fij=1\u03b4ijf=1 Kontribusi dari atribut ff untuk dissimilarity antara i dan j (yaitu.dfijdijf) dihitung bergantung pada tipenya, Jika ff adalah numerik, $$ d_{ij}^{f}=\\frac{ |x {if}-x {jf}|}{max_hx_{hf}-min_hx{hf}} $$ , di mana h menjalankan semua nilai objek yang tidak hilang untuk atribut f Jika ff adalah nominal atau binary,$d_{ij}^{f}=0 $jika xif=xjfxif=xjf, sebaliknya dfij=1dijf=1 Jika ff adalah ordinal maka hitung rangking rifrif dan $$ \\mathcal z_{if}=\\frac {r_{if}-1}{M_f-1} $$ , dan perlakukan zifzif sebagai numerik. Contoh Program import pandas as pd pd.read_csv(\"hayes-roth.csv\") name hobby age educationallevel maritalstatus class 0 92 2 1 1 2 1 1 10 2 1 3 2 2 2 83 3 1 4 1 3 3 61 2 4 2 2 3 4 107 1 1 3 4 3 ... ... ... ... ... ... ... 127 44 1 1 4 3 3 128 40 2 1 2 1 1 129 90 1 2 1 2 2 130 21 1 2 2 1 2 131 9 3 1 1 2 1 132 rows \u00d7 6 columns Menghitung Jarak Numerik Numeric attribute adalah atribut kuantitatif, yaitu dapat dihitung banyaknya dan dapat diwakilkan dalam bilangan integer atau real. Numeric attribute dapat berupa skala interval(interval-scaled) atau skala-rasio(ration-scaled), kedua-duanya memiliki nilai mean, median, dan mode. Untuk numeric attribute interval-scaled tidak memiliki true zero-point(nilai nol yang sebenarnya), sedangkan yang ratio-scaled memiliki true zero-point. def chordDist(v1,v2,jenis): jumlah=0 normv1=0 normv2=0 for x in range (len(jenis)): normv1=normv1+(int(a.values.tolist()[v1][jenis[x]])**2) normv2=normv2+(int(a.values.tolist()[v2][jenis[x]])**2) jumlah=jumlah+(int(a.values.tolist()[v1][jenis[x]])*int(a.values.tolist()[v2][jenis[x]])) return ((2-(2*jumlah/(normv1*normv2)))**0.5) Menghitung Jarak Binary Binary attribute adalah nominal atribut yang hanya memiliki dua kategori atau keadaan yaitu 0 dan 1. 0 berarti tidak ada dan 1 berarti ada. Binary attribute biasanya diartikan sebagai Boolean jika kedua keadaannya adalah true(benar) dan false(salah). Binary attribute bisa simetris(symmetric) dan bisa asimetris(assymmetric). Simetris jika kedua nilainya bernilai sama/setimbang harganya, sehingga tidak bisa diberi kode 0 atau 1, sedangkan asimetris kedua nilainya tidak setimbang harganya, sehingga dapat diberi kode 0 atau 1. def binaryDist(v1,v2,jenis): q=0 r=0 s=0 t=0 for x in range (len(jenis)): if (int(a.values.tolist()[v1][jenis[x]]))==1 and (int(a.values.tolist()[v2][jenis[x]]))==1: q=q+1 elif (int(a.values.tolist()[v1][jenis[x]]))==1 and (int(a.values.tolist()[v2][jenis[x]]))==2: r=r+1 elif (int(a.values.tolist()[v1][jenis[x]]))==2 and (int(a.values.tolist()[v2][jenis[x]]))==1: s=s+1 else: t=t+1 return ((r+s)/(q+r+s+t)) Menghitung Jarak Ordinal Ordinal attribute adalah atribut dengan nilai-nilai yang kemungkinan memiliki urutan yang mempunyai arti atau tingkatan(ranking), akan tetapi jarak antara nilai-nilainya tidak diketahui. Ordinal attribute berguna untuk mendaftarkan taksiran suatu kualitas yang tidak bisa diukur secara obyektif. Oleh karena itu ordinal attribute biasanya digunakan dalam survey atau rating. def ordDist(v1,v2,jenis): jumlah=0 for x in range (len(jenis)): z1=int(a.values.tolist()[v1][jenis[x]])-1 z2=int(a.values.tolist()[v2][jenis[x]])-1 jumlah=jumlah+chordDist(z1,z2,jenis) return (jumlah) Atribut Nominal Nominal attribute adalah atribut yang nilainya berupa simbol-simbol atau nama-nama benda. Dan nilainya tidak memiliki urutan yang memiliki arti. Pada nominal attribute, operasi matematika pada nilai-nilainya tidak berarti. Sehingga, tidak masuk akal untuk mencari nilai mean(rata-rata)nya atau nilai median(tengah) nya, kecuali untuk mode(nilai yang paling sering muncul)nya. Sumber : https://github.com/mulaab/datamining/tree/master/memahami-data https://datamining10041.wordpress.com/2012/03/25/atribut-nominal-biner-ordinal-dan-numerik/","title":"Tugas 2"},{"location":"tugas2/#tugas-2","text":"","title":"Tugas 2"},{"location":"tugas2/#mengukur-jarak-data","text":"","title":"Mengukur Jarak Data"},{"location":"tugas2/#mengukur-jarak-tipe-numerik","text":"Salah satu tantangan dalam era ini dengan datatabase yang memiliki banyak tipe data. Mengukur jarak adalah komponen utama dalam algoritma clustering berbasis jarak. Alogritma seperit Algoritma Partisioning misal K-Mean, K-medoidm dan fuzzy c-mean dan rough clustering bergantung pada jarak untuk melakukan pengelompokkan Sebelum menjelaskan tentang beberapa macam ukuran jarak, kita mendefinisikan terlebih dahulu yaiut v1,v2v1,v2 menyatakan dua vektor yang menyatakan v1=x1,x2,...,xn,v2=y1,y2,...,yn,v1=x1,x2,...,xn,v2=y1,y2,...,yn, dimana xi,yixi,yi disebut attribut. Ada beberapa ukuran similaritas datau ukuran jarak, diantaranya :","title":"Mengukur Jarak Tipe Numerik"},{"location":"tugas2/#minkowski-distance","text":"Kelompok Minkowski diantaranya adalah Euclidean distance dan Manhattan distance, yang menjadi kasus khusus dari Minkowski distance. Minkowski distance dinyatakan dengan dimana m adalah bilangan riel positif dan x i dan y i adalah dua vektor dalam runang dimensi nn Implementasi ukuran jarak Minkowski pada model clustering data atribut dilakukan normalisasi untuk menghindari dominasi dari atribut yang memiliki skala data besar.","title":"Minkowski Distance"},{"location":"tugas2/#manhattan-distance","text":"Manhattan distance adalah kasus khsusu dari jarak Minkowski distance pada m = 1. Seperti Minkowski Distance, Manhattan distance sensitif terhadap outlier. BIla ukuran ini digunakan dalam algoritma clustering , bentuk cluster adalah hyper-rectangular. Ukuran ini didefinisikan dengan","title":"Manhattan distance"},{"location":"tugas2/#euclidean-distance","text":"Jarak yang paling terkenal yang digunakan untuk data numerik adalah jarak Euclidean. Ini adalah kasus khusus dari jarak Minkowski ketika m = 2. Jarak Euclidean berkinerja baik ketika digunakan untuk kumpulan data cluster kompak atau terisolasi . Meskipun jarak Euclidean sangat umum dalam pengelompokan, ia memiliki kelemahan: jika dua vektor data tidak memiliki nilai atribut yang sama, kemungkin memiliki jarak yang lebih kecil daripada pasangan vektor data lainnya yang mengandung nilai atribut yang sama. Masalah lain dengan jarak Euclidean sebagai fitur skala terbesar akan mendominasi yang lain. Normalisasi fitur kontinu adalah solusi untuk mengatasi kelemahan ini.","title":"Euclidean distance"},{"location":"tugas2/#average-distance","text":"Berkenaan dengan kekurangan dari Jarak Euclidian Distance diatas, rata rata jarak adala versi modikfikasid ari jarak Euclidian untuk memperbaiki hasil. Untuk dua titik x,yx,y dalam ruang dimensi nn, rata-rata jarak didefinisikan dengan","title":"Average Distance"},{"location":"tugas2/#weighted-euclidean-distance","text":"Jika berdasarkan tingkatan penting dari masing masing atribut ditentukan, maka Weighted Euclidean distance adalah modifikisasi lain dari jarak Euclidean distance yang dapat digunakan. Ukuran ini dirumuskan dengan dimana wi adalah bobot yang diberikan pada atribut ke i.","title":"Weighted euclidean distance"},{"location":"tugas2/#chord-distance","text":"Chord distance adalah salah satu ukuran jarak modifikasi Euclidean distance untuk mengatasi kekurangan dari Euclidean distance. Ini dapat dipecahkan juga dengan menggunakan skala pengukuran yang baik. Jarak ini dapat juga dihitung dari data yang tidak dinormalisasi . Chord distance didefinisikan dengan dimana \u2016 x \u20162 adalah L 2-norm .","title":"Chord distance"},{"location":"tugas2/#mahalanobis-distance","text":"Mahalanobis distance berdasarkan data berbeda dengan Euclidean dan Manhattan distances yang bebas antra data dengan data yang lain. Jarak Mahalanobis yang teratur dapat digunakan untuk mengekstraksi hyperellipsoidal clusters. Jarak Mahalanobis dapat mengurangi distorsi yang disebabkan oleh korelasi linier antara fitur dengan menerapkan transformasi pemutihan ke data atau dengan menggunakan kuadrat Jarak mahalanobis. Mahalanobis distance dinyatakan dengan dimana S adalah matrik covariance data.","title":"Mahalanobis distance"},{"location":"tugas2/#cosine-measure","text":"Ukuran Cosine similarity lebih banyak digunakan dalam similaritas dokumen dan dinyatakan dengan dimana \u2225y\u22252 adalah Euclidean norm dari vektor y=(y1,y2,\u2026,yn)y=(y1,y2,\u2026,yn) didefinisikan dengan","title":"Cosine measure"},{"location":"tugas2/#pearson-correlation","text":"Pearson correlation banyak digunakan dalam data expresi gen. Ukuran similaritas ini menghitung similaritas antara duan bentuk pola expresi gen. Pearson correlation didefinisikan dengan , where \u03bc x The Pearson correlation kelemahannya adalah sensitif terhadap outlier","title":"Pearson correlation"},{"location":"tugas2/#mengukur-jarak-atribut-binary","text":"Mari kita lihat similaritas dan desimilirity untuk objek yang dijelaskan oleh atribut biner simetris atau asimetris. Aatribut biner hanya memiliki dua status: 0 dan 1 Contoh atribut perokok menggambarkan seorang pasien, misalnya, 1 menunjukkan bahwa pasien merokok, sedangkan 0 menunjukkan pasien tidak merokok. Memperlakukan atribut biner sebagai atribut numerik tidak diperkenankan. Oleh karena itu, metode khusus untuk data biner diperlukan untuk membedakan komputasi. Jadi, bagaimana kita bisa menghitung ketidaksamaan antara dua atribut biner? \u201dSatu pendekatan melibatkan penghitungan matriks ketidaksamaan dari data biner yang diberikan. Jika semua atribut biner dianggap memiliki bobot yang sama, kita memiliki tabel kontingensi 2\u00d72 di mana qq adalah jumlah atribut yang sama dengan 1 untuk kedua objek ii dan jj, rr adalah jumlah atribut yang sama dengan 1 untuk objek ii tetapi 0 untuk objek jj, ss adalah jumlah atribut yang sama dengan 0 untuk objek ii tetapi 1 untuk objek jj, dan tt adalah jumlah atribut yang sama dengan 0 untuk kedua objek ii dan jj. Jumlah total atribut adalah pp, di mana p=q+r+s+tp=q+r+s+t Ingatlah bahwa untuk atribut biner simetris, masing-masing nilai bobot yang sama.Dissimilarity yang didasarkan pada atribut aymmetric binary disebut symmetric binary dissimilarity. Jika objek i dan j dinyatakan sebagai atribut biner simetris, maka dissimilarity antarii dan j adalah $$ d ( i , j ) = \\frac { r + s } { q + r + s + t } $$ Untuk atribut biner asimetris, kedua kondisi tersebut tidak sama pentingnya, seperti hasil positif (1) dan negatif (0) dari tes penyakit. Diberikan dua atribut biner asimetris, pencocokan keduanya 1 (kecocokan positif) kemudian dianggap lebih signifikan daripada kecocokan negatif. Ketidaksamaan berdasarkan atribut-atribut ini disebut asimetris biner dissimilarity, di mana jumlah kecocokan negatif, t, dianggap tidak penting dan dengan demikian diabaikan. Berikut perhitungannya $$ d ( i , j ) = \\frac { r + s } { q + r + s } $$ Kita dapat mengukur perbedaan antara dua atribut biner berdasarkan pada disimilarity. Misalnya, biner asimetris kesamaan antara objek ii dan jj dapat dihitung dengan $$ \\operatorname { sim } ( i , j ) = \\frac { q } { q + r + s } = 1 - d ( i , j ) $$ Persamaan similarity ini disebut dengan Jaccard coefficient","title":"Mengukur Jarak Atribut Binary"},{"location":"tugas2/#mengukur-jarak-tipe-categorical","text":"Li, C., & Li, H. (2010). A Survey of Distance Metrics for Nominal Attributes. JSW, 5(11), 1262-1269.","title":"Mengukur Jarak Tipe categorical"},{"location":"tugas2/#overlay-metric","text":"Ketika semua atribut adalah bertipe nominal, ukuran jarak yang paling sederhana adalah dengan Ovelay Metric (OM) yang dinyatakan dengan $$ d ( x , y ) = \\sum _ { i = 1 } ^ { n } \\delta ( a _ { i } ( x ) , a _ { i } ( y ) ) $$ dimana nn adalah banyaknya atribut, ai(x)ai(x) dan ai(y)ai(y) adalah nilai atribut ke ii yaitu AiAi dari masing masing objek xx dan yy, \u03b4 (ai(x),ai(y))\u03b4 (ai(x),ai(y)) adalah 0 jika ai(x)=ai(y) dan 1 jika sebaliknya. OM banyak digunakan oleh instance-based learning dan locally weighted learning. Jelas sekali , ini sedikit beruk untuk mengukur jarak antara masing-masing pasangan sample, karena gagal memanfaatkan tambahan informasi yang diberikan oleh nilai atribut nominal yang bisa membantu dalam generalisasi.","title":"Overlay Metric"},{"location":"tugas2/#value-difference-metric-vdm","text":"VDM dikenalkan oleh Standfill and Waltz, versi sederhana dari VDM tanpa skema pembobotan didefinsisikan dengan $$ d ( x , y ) = \\sum _ { i = 1 } ^ { n } \\sum _ { c = 1 } ^ { C } \\left| P ( c | a _ { i } ( x ) ) - P ( c | a _ { i } ( y ) ) \\right | $$ dimana CCadalah banyaknya kelas, P(c|ai(x))P(c|ai(x)) adalah probabilitas bersyarat dimana kelas xx adalah cc dari atribut AiAi, yang memiliki nilai ai(x)ai(x), P(c|ai(y))P(c|ai(y)) adalah probabilitas bersyarat dimana kelas yy adalah cc dengan atribut AiAi memiliki nilai ai(y)ai(y) VDM mengasumsikan bahwa dua nilai dari atribut adalah lebih dekat jika memiliki klasifikasi sama. Pendekatan lain berbasi probabilitas adalah SFM (Short and Fukunaga Metric) yang kemudian dikembangkan oleh Myles dan Hand dan didefinisikan dengan $$ d ( x , y ) = \\sum _ { c = 1 } ^ { C } \\left | P ( c | x ) - P ( c | y ) \\right| $$ diman probabilitas keanggotaan kelas diestimasi dengan P(c|x) dan P(c|y) didekati dengan Naive Bayes,","title":"Value Difference Metric (VDM)"},{"location":"tugas2/#minimum-risk-metric-mrm","text":"Ukuran ini dipresentasikan oleh Blanzieri and Ricci, berbeda dari SFM yaitu meminimumkan selisih antara kesalahan berhingga dan kesalahan asymtotic. MRM meminimumkan risk of misclassification yang didefinisikan dengan $$ d ( x , y ) = \\sum _ { c = 1 } ^ { C } P ( c | x ) ( 1 - P ( c | y ) ) $$","title":"Minimum Risk Metric (MRM)"},{"location":"tugas2/#mengukur-jarak-tipe-ordinal","text":"Han, J., Pei, J., & Kamber, M. (2011). Data mining: concepts and techniques. Elsevier . Nilai-nilai atribut ordinal memiliki urutan atau peringkat, namun besarnya antara nilai-nilai berturut-turut tidak diketahui. Contohnya tingkatan kecil, sedang, besar untuk atribut ukuran. Atribut ordinal juga dapat diperoleh dari diskritisasi atribut numerik dengan membuat rentang nilai ke dalam sejumlah kategori tertentu. Kategori-kategori ini disusun dalam peringkat. Yaitu, rentang atribut numerik dapat dipetakan ke atribut ordinal ff yang memiliki MfMf state. Misalnya, kisaran suhu atribut skala-skala (dalam Celcius)dapat diatur ke dalam status berikut: \u221230 hingga \u221210, \u221210 hingga 10, 10 hingga 30, masing-masing mewakili kategori suhu dingin, suhu sedang, dan suhu hangat. MM adalah jumlah keadaan yang dapat dilakukan oleh atribut ordinalmemiliki. State ini menentukan peringkat 1,...,Mf1,...,Mf Perlakuan untuk atribut ordinal adalah cukup sama dengan atribut numerik ketika menghitung disimilarity antara objek. Misalkan ff adalah atribut-atribut dari atribut ordinal dari nn objek. Menghitung disimilarity terhadap f fitur sebagai berikut: Nilai ff untuk objek ke-ii adalah xifxif, dan ff memiliki MfMf status urutan , mewakili peringkat 1,..,Mf1,..,Mf Ganti setiap xifxif dengan peringkatnya, rif\u2208{1...Mf}rif\u2208{1...Mf} Karena setiap atribut ordinal dapat memiliki jumlah state yang berbeda, diperlukan untuk memetakan rentang setiap atribut ke [0,0, 1.0] sehingga setiap atribut memiliki bobot yang sama. Perl melakukan normalisasi data dengan mengganti peringkat rifrif dengan $$ z _ { i f } = \\frac { r _ { i f } - 1 } { M _ { f } - 1 } $$ Dissimilarity kemudian dihitung dengan menggunakan ukuran jarak seperti atribut numerik dengan data yang baru setelah ditransformasi $ z _ { i f }$","title":"Mengukur Jarak Tipe Ordinal"},{"location":"tugas2/#menghitung-jarak-tipe-campuran","text":"Wilson, D. R., & Martinez, T. R. (1997). Improved heterogeneous distance functions. Journal of artificial intelligence research, 6, 1-34. Menghitung ketidaksamaan antara objek dengan atribut campuran yang berupa nominal, biner simetris, biner asimetris, numerik, atau ordinal yang ada pada kebanyakan databasae dapat dinyatakan dengan memproses semua tipe atribut secara bersamaan. Salah satu teknik tersebut menggabungkan atribut yang berbeda ke dalam matriks ketidaksamaan tunggal dan menyatakannya dengan skala interval antar [0,0,1.0][0,0,1.0]. Misalkan data berisi atribut pp tipe campuran. Ketidaksamaan (disimilarity ) antara objek ii dan jj dinyatakan dengan $$ d ( i , j ) = \\frac { \\sum _ { f = 1 } ^ { p } \\delta _ { i j } ^ { ( f ) } d _ { i j } ^ { ( f ) } } { \\sum _ { f = 1 } ^ { p } \\delta _ { i j } ^ { ( f ) } } $$ dimana \u03b4fij=0\u03b4ijf=0 - jika xifxif atau xjfxjf adalah hilang (i.e., tidak ada pengukuran dari atribut f untuk objek ii atau objek jj) jika xif=xjf=0xif=xjf=0 dan atribut ff adalah binary asymmetric, selain itu \u03b4fij=1\u03b4ijf=1 Kontribusi dari atribut ff untuk dissimilarity antara i dan j (yaitu.dfijdijf) dihitung bergantung pada tipenya, Jika ff adalah numerik, $$ d_{ij}^{f}=\\frac{ |x {if}-x {jf}|}{max_hx_{hf}-min_hx{hf}} $$ , di mana h menjalankan semua nilai objek yang tidak hilang untuk atribut f Jika ff adalah nominal atau binary,$d_{ij}^{f}=0 $jika xif=xjfxif=xjf, sebaliknya dfij=1dijf=1 Jika ff adalah ordinal maka hitung rangking rifrif dan $$ \\mathcal z_{if}=\\frac {r_{if}-1}{M_f-1} $$ , dan perlakukan zifzif sebagai numerik.","title":"Menghitung Jarak Tipe Campuran"},{"location":"tugas2/#contoh-program","text":"import pandas as pd pd.read_csv(\"hayes-roth.csv\") name hobby age educationallevel maritalstatus class 0 92 2 1 1 2 1 1 10 2 1 3 2 2 2 83 3 1 4 1 3 3 61 2 4 2 2 3 4 107 1 1 3 4 3 ... ... ... ... ... ... ... 127 44 1 1 4 3 3 128 40 2 1 2 1 1 129 90 1 2 1 2 2 130 21 1 2 2 1 2 131 9 3 1 1 2 1 132 rows \u00d7 6 columns","title":"Contoh Program"},{"location":"tugas2/#menghitung-jarak-numerik","text":"Numeric attribute adalah atribut kuantitatif, yaitu dapat dihitung banyaknya dan dapat diwakilkan dalam bilangan integer atau real. Numeric attribute dapat berupa skala interval(interval-scaled) atau skala-rasio(ration-scaled), kedua-duanya memiliki nilai mean, median, dan mode. Untuk numeric attribute interval-scaled tidak memiliki true zero-point(nilai nol yang sebenarnya), sedangkan yang ratio-scaled memiliki true zero-point. def chordDist(v1,v2,jenis): jumlah=0 normv1=0 normv2=0 for x in range (len(jenis)): normv1=normv1+(int(a.values.tolist()[v1][jenis[x]])**2) normv2=normv2+(int(a.values.tolist()[v2][jenis[x]])**2) jumlah=jumlah+(int(a.values.tolist()[v1][jenis[x]])*int(a.values.tolist()[v2][jenis[x]])) return ((2-(2*jumlah/(normv1*normv2)))**0.5)","title":"Menghitung Jarak Numerik"},{"location":"tugas2/#menghitung-jarak-binary","text":"Binary attribute adalah nominal atribut yang hanya memiliki dua kategori atau keadaan yaitu 0 dan 1. 0 berarti tidak ada dan 1 berarti ada. Binary attribute biasanya diartikan sebagai Boolean jika kedua keadaannya adalah true(benar) dan false(salah). Binary attribute bisa simetris(symmetric) dan bisa asimetris(assymmetric). Simetris jika kedua nilainya bernilai sama/setimbang harganya, sehingga tidak bisa diberi kode 0 atau 1, sedangkan asimetris kedua nilainya tidak setimbang harganya, sehingga dapat diberi kode 0 atau 1. def binaryDist(v1,v2,jenis): q=0 r=0 s=0 t=0 for x in range (len(jenis)): if (int(a.values.tolist()[v1][jenis[x]]))==1 and (int(a.values.tolist()[v2][jenis[x]]))==1: q=q+1 elif (int(a.values.tolist()[v1][jenis[x]]))==1 and (int(a.values.tolist()[v2][jenis[x]]))==2: r=r+1 elif (int(a.values.tolist()[v1][jenis[x]]))==2 and (int(a.values.tolist()[v2][jenis[x]]))==1: s=s+1 else: t=t+1 return ((r+s)/(q+r+s+t))","title":"Menghitung Jarak Binary"},{"location":"tugas2/#menghitung-jarak-ordinal","text":"Ordinal attribute adalah atribut dengan nilai-nilai yang kemungkinan memiliki urutan yang mempunyai arti atau tingkatan(ranking), akan tetapi jarak antara nilai-nilainya tidak diketahui. Ordinal attribute berguna untuk mendaftarkan taksiran suatu kualitas yang tidak bisa diukur secara obyektif. Oleh karena itu ordinal attribute biasanya digunakan dalam survey atau rating. def ordDist(v1,v2,jenis): jumlah=0 for x in range (len(jenis)): z1=int(a.values.tolist()[v1][jenis[x]])-1 z2=int(a.values.tolist()[v2][jenis[x]])-1 jumlah=jumlah+chordDist(z1,z2,jenis) return (jumlah)","title":"Menghitung Jarak Ordinal"},{"location":"tugas2/#atribut-nominal","text":"Nominal attribute adalah atribut yang nilainya berupa simbol-simbol atau nama-nama benda. Dan nilainya tidak memiliki urutan yang memiliki arti. Pada nominal attribute, operasi matematika pada nilai-nilainya tidak berarti. Sehingga, tidak masuk akal untuk mencari nilai mean(rata-rata)nya atau nilai median(tengah) nya, kecuali untuk mode(nilai yang paling sering muncul)nya. Sumber : https://github.com/mulaab/datamining/tree/master/memahami-data https://datamining10041.wordpress.com/2012/03/25/atribut-nominal-biner-ordinal-dan-numerik/","title":"Atribut Nominal"},{"location":"tugas3/","text":"Tugas 3 Seleksi Fitur Seleksi fitur merupakan teknik untuk mengurangi dimensi atribut. Pengurangan dimensi tersebut dilakukan untuk mendapatkan atribut-atribut yang relevan dan tidak berlebihan sehingga dapat mempercepat proses klasifikasi dan dapat meningkatkan akurasi dari algoritme klasifikasi. (Arifin, 2015). Metode seleksi fitur yang biasa digunakan dalam penelitian adalah Information gain. Information Gain Information Gain merupakan metode seleksi fitur paling sederhana dengan melakukan perangkingan atribut dan banyak digunakan dalam aplikasi kategorisasi teks, analisis data microarray dan analisis data citra. (Chormunge & Jena, 2016). Information Gain dapat membantu mengurangi noise yang disebabkan oleh fitur-fitur yang tidak relevan. Information Gain mendeteksi fitur-fitur yang paling banyak memiliki informasi berdasarkan kelas tertentu. Penentuan atribut terbaik dilakukan dengan menghitung nilai entropy terlebih dahulu. Entropy Entropy merupakan ukuran ketidakpastian kelas dengan menggunakan probabilitas kejadian atau atribut tertentu. (Shaltout, et al., 2014). Rumus untuk menghitung entropy ditunjukkan pada persamaan (1). Setelah mendapatkan nilai entropy, maka perhitungan Information Gain dapat dilakukan dengan menggunakan persamaan (2). (Firmahsyah & Gantini, 2016). $$ E(T) = \\sum_{i=1}^n {-P_i\\log{P_i}} $$ Dengan n adalah jumlah nilai yang ada pada kelas klasifikasi dan Pi merupakan jumlah sampel untuk kelas i. $$ \\operatorname{Gain}(S, A) = \\operatorname{Entropy}(S) - \\sum_{v\\in{A}} \\frac{S_{X,v}}{S} E(S_{X,v}) $$ Dengan A merupakan atribut, v adalah nilai yang mungkin untuk atribut A, Values(A) adalah himpunan nilai-nilai yang mungkin untuk A, |Sv| adalah jumlah seluruh sampel data dan Entropy(Sv) adalah entropy untuk sampel-sampel yang memiliki nilai v. Contoh program Data tentang kondisi cuaca : from pandas import * from IPython.display import HTML, display from tabulate import tabulate from math import log from sklearn.feature_selection import mutual_info_classif def table(df): display(HTML(tabulate(df, tablefmt='html', headers='keys', showindex=False))) df = read_csv('featureselection.csv',usecols = [0,1,2,3,4], sep=';') table(df) Tampilan : outlook temperature humidity windy play sunny hot high False no sunny hot high True no overcast hot high False yes rainy mild high False yes rainy cool normal False yes rainy cool normal True no overcast cool normal True yes sunny mild high False no sunny cool normal False yes rainy mild normal False yes sunny mild normal True yes overcast mild high True yes overcast hot normal False yes rainy mild high True no Menghitung Entropy def findEntropy(column): rawGroups = df.groupby(column) targetGroups = [[key, len(data), len(data)/df[column].size] for key,data in rawGroups] targetGroups = DataFrame(targetGroups, columns=['play', 'count', 'probability']) return sum([-x*log(x,2) for x in targetGroups['probability']]), targetGroups, rawGroups entropyTarget, groupTargets, _ = findEntropy('play') table(groupTargets) print('Entropy =', entropyTarget) Tampilan : play count probability no 5 0.357143 yes 9 0.642857 Entropy = 0.9402859586706309 Menghitung Gain def findGain(column): entropyOutlook, groupOutlooks, rawOutlooks = findEntropy(column) table(groupOutlooks) gain = entropyTarget-sum(len(data)/len(df)*sum(-x/len(data)*log(x/len(data),2) for x in data.groupby('play').size()) for key,data in rawOutlooks) print(\"Gain of\",column,\"is\",gain) return gain gains = [[x,findGain(x)] for x in ['outlook','temperature','humidity','windy']] Tampilan : value count probability overcast 4 0.285714 rainy 5 0.357143 sunny 5 0.357143 gain of outlook is 0.2467498197744391 value count probability cool 4 0.285714 hot 4 0.285714 mild 6 0.428571 gain of temperature is 0.029222565658954647 value count probability high 7 0.5 normal 7 0.5 gain of humidity is 0.15183550136234136 value count probability False 8 0.571429 True 6 0.428571 gain of windy is 0.04812703040826927 Ranking Fitur Hasil dari pengurutan gain dari yang terbesar ke yang terkecil, digunakan untuk menghilangkan fitur yang kurang penting / relevan. table(DataFrame(gains, columns=[\"Feature\", \"Gain Score\"]).sort_values(\"Gain Score\")[::-1]) Tampilan : Feature Gain Score outlook 0.24675 humidity 0.151836 windy 0.048127 temperature 0.0292226 Sumber : https://www.researchgate.net/publication/326571453_Seleksi_Fitur_Information_Gain_untuk_Klasifikasi_Penyakit_Jantung_Menggunakan_Kombinasi_Metode_K-Nearest_Neighbor_dan_Naive_Bayes MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} });","title":"Tugas 3"},{"location":"tugas3/#tugas-3","text":"","title":"Tugas 3"},{"location":"tugas3/#seleksi-fitur","text":"Seleksi fitur merupakan teknik untuk mengurangi dimensi atribut. Pengurangan dimensi tersebut dilakukan untuk mendapatkan atribut-atribut yang relevan dan tidak berlebihan sehingga dapat mempercepat proses klasifikasi dan dapat meningkatkan akurasi dari algoritme klasifikasi. (Arifin, 2015). Metode seleksi fitur yang biasa digunakan dalam penelitian adalah Information gain.","title":"Seleksi Fitur"},{"location":"tugas3/#information-gain","text":"Information Gain merupakan metode seleksi fitur paling sederhana dengan melakukan perangkingan atribut dan banyak digunakan dalam aplikasi kategorisasi teks, analisis data microarray dan analisis data citra. (Chormunge & Jena, 2016). Information Gain dapat membantu mengurangi noise yang disebabkan oleh fitur-fitur yang tidak relevan. Information Gain mendeteksi fitur-fitur yang paling banyak memiliki informasi berdasarkan kelas tertentu. Penentuan atribut terbaik dilakukan dengan menghitung nilai entropy terlebih dahulu.","title":"Information Gain"},{"location":"tugas3/#entropy","text":"Entropy merupakan ukuran ketidakpastian kelas dengan menggunakan probabilitas kejadian atau atribut tertentu. (Shaltout, et al., 2014). Rumus untuk menghitung entropy ditunjukkan pada persamaan (1). Setelah mendapatkan nilai entropy, maka perhitungan Information Gain dapat dilakukan dengan menggunakan persamaan (2). (Firmahsyah & Gantini, 2016). $$ E(T) = \\sum_{i=1}^n {-P_i\\log{P_i}} $$ Dengan n adalah jumlah nilai yang ada pada kelas klasifikasi dan Pi merupakan jumlah sampel untuk kelas i. $$ \\operatorname{Gain}(S, A) = \\operatorname{Entropy}(S) - \\sum_{v\\in{A}} \\frac{S_{X,v}}{S} E(S_{X,v}) $$ Dengan A merupakan atribut, v adalah nilai yang mungkin untuk atribut A, Values(A) adalah himpunan nilai-nilai yang mungkin untuk A, |Sv| adalah jumlah seluruh sampel data dan Entropy(Sv) adalah entropy untuk sampel-sampel yang memiliki nilai v.","title":"Entropy"},{"location":"tugas3/#contoh-program","text":"Data tentang kondisi cuaca : from pandas import * from IPython.display import HTML, display from tabulate import tabulate from math import log from sklearn.feature_selection import mutual_info_classif def table(df): display(HTML(tabulate(df, tablefmt='html', headers='keys', showindex=False))) df = read_csv('featureselection.csv',usecols = [0,1,2,3,4], sep=';') table(df) Tampilan : outlook temperature humidity windy play sunny hot high False no sunny hot high True no overcast hot high False yes rainy mild high False yes rainy cool normal False yes rainy cool normal True no overcast cool normal True yes sunny mild high False no sunny cool normal False yes rainy mild normal False yes sunny mild normal True yes overcast mild high True yes overcast hot normal False yes rainy mild high True no Menghitung Entropy def findEntropy(column): rawGroups = df.groupby(column) targetGroups = [[key, len(data), len(data)/df[column].size] for key,data in rawGroups] targetGroups = DataFrame(targetGroups, columns=['play', 'count', 'probability']) return sum([-x*log(x,2) for x in targetGroups['probability']]), targetGroups, rawGroups entropyTarget, groupTargets, _ = findEntropy('play') table(groupTargets) print('Entropy =', entropyTarget) Tampilan : play count probability no 5 0.357143 yes 9 0.642857 Entropy = 0.9402859586706309 Menghitung Gain def findGain(column): entropyOutlook, groupOutlooks, rawOutlooks = findEntropy(column) table(groupOutlooks) gain = entropyTarget-sum(len(data)/len(df)*sum(-x/len(data)*log(x/len(data),2) for x in data.groupby('play').size()) for key,data in rawOutlooks) print(\"Gain of\",column,\"is\",gain) return gain gains = [[x,findGain(x)] for x in ['outlook','temperature','humidity','windy']] Tampilan : value count probability overcast 4 0.285714 rainy 5 0.357143 sunny 5 0.357143 gain of outlook is 0.2467498197744391 value count probability cool 4 0.285714 hot 4 0.285714 mild 6 0.428571 gain of temperature is 0.029222565658954647 value count probability high 7 0.5 normal 7 0.5 gain of humidity is 0.15183550136234136 value count probability False 8 0.571429 True 6 0.428571 gain of windy is 0.04812703040826927","title":"Contoh program"},{"location":"tugas3/#ranking-fitur","text":"Hasil dari pengurutan gain dari yang terbesar ke yang terkecil, digunakan untuk menghilangkan fitur yang kurang penting / relevan. table(DataFrame(gains, columns=[\"Feature\", \"Gain Score\"]).sort_values(\"Gain Score\")[::-1]) Tampilan : Feature Gain Score outlook 0.24675 humidity 0.151836 windy 0.048127 temperature 0.0292226 Sumber : https://www.researchgate.net/publication/326571453_Seleksi_Fitur_Information_Gain_untuk_Klasifikasi_Penyakit_Jantung_Menggunakan_Kombinasi_Metode_K-Nearest_Neighbor_dan_Naive_Bayes MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} });","title":"Ranking Fitur"},{"location":"tugas4/","text":"Tugas 4 Naive Bayes Classifier Na\u00efve Bayes merupakan sebuah pengklasifikasian probabilistik sederhana yang menghitung sekumpulan probabilitas dengan menjumlahkan frekuensi dan kombinasi nilai dari dataset yang diberikan. Algoritma mengunakan teorema Bayes dan mengasumsikan semua atribut independen atau tidak saling ketergantungan yang diberikan oleh nilai pada variabel kelas ( Patil and Sherekar 2013 ). Definisi lain mengatakan Na\u00efve Bayes merupakan pengklasifikasian dengan metode probabilitas dan statistik yang dikemukan oleh ilmuwan Inggris Thomas Bayes, yaitu memprediksi peluang di masa depan berdasarkan pengalaman di masa sebelumnya ( Bustami 2013 ). Na\u00efve Bayes didasarkan pada asumsi penyederhanaan bahwa nilai atribut secara kondisional saling bebas jika diberikan nilai output. Dengan kata lain, diberikan nilai output, probabilitas mengamati secara bersama adalah produk dari probabilitas individu ( Ridwan 2013 ). Keuntungan penggunaan Naive Bayes adalah bahwa metode ini hanya membutuhkan jumlah data pelatihan (Training Data) yang kecil untuk menentukan estimasi paremeter yang diperlukan dalam proses pengklasifikasian. Naive Bayes sering bekerja jauh lebih baik dalam kebanyakan situasi dunia nyata yang kompleks dari pada yang diharapkan ( Pattekari and Parveen 2012 ) Naive Bayes Classifier dinilai bekerja sangat baik dibanding dengan model classifier lainnya, yaitu Na\u00efve Bayes Classifier memiliki tingkat akurasi yg lebih baik dibanding model classifier lainnya( Xhemali 2009 ). Teorema Na\u00efve Bayes Sebelum menjelaskan Na\u00efve Bayes Classifier ini, akan dijelaskan terlebih dahulu Teorema Bayes yang menjadi dasar dari metoda tersebut. Pada Teorema Bayes , bila terdapat dua kejadian yang terpisah (misalkan X dan H ), maka Teorema Bayes dirumuskan sebagai berikut ( Bustami 2013 ).: .. [NBC-01] Keterangan Teorema Bayes sering pula dikembangkan mengingat berlakunya hukum probabilitas total, menjadi seperti berikut: .. [NBC-02] Keterangan Untuk menjelaskan Teorema Na\u00efve Bayes , perlu diketahui bahwa proses klasifikasi memerlukan sejumlah petunjuk untuk menentukan kelas apa yang cocok bagi sampel yang dianalisis tersebut. Karena itu, Teorema Bayes di atas disesuaikan sebagai berikut: .. [NBC-03] Di mana Variabel C merepresentasikan kelas, sementara variabel F1 ... Fn merepresentasikan karakteristik petunjuk yang dibutuhkan untuk melakukan klasifikasi. Maka rumus tersebut menjelaskan bahwa peluang masuknya sampel karakteristik tertentu dalam kelas C ( Posterior ) adalah peluang munculnya kelas C (sebelum masuknya sampel tersebut, seringkali disebut prior ), dikali dengan peluang kemunculan karakteristik-karakteristik sampel pada kelas C (disebut juga likelihood ), dibagi dengan peluang kemunculan karakteristik-karakteristik sampel secara global (disebut juga evidence ). Karena itu, rumus di atas dapat pula ditulis secara sederhana sebagai berikut: .. [NBC-04] Nilai Evidence selalu tetap untuk setiap kelas pada satu sampel . Nilai dari posterior tersebut nantinya akan dibandingkan dengan nilai-nilai posterior kelas lainnya untuk menentukan ke kelas apa suatu sampel akan diklasifikasikan. Penjabaran lebih lanjut rumus Bayes tersebut dilakukan dengan menjabarkan (C,F1, ... , Fn) menggunakan aturan perkalian sebagai berikut: .. [NBC-05] Dapat dilihat bahwa hasil penjabaran tersebut menyebabkan semakin banyak dan semakin kompleksnya faktor - faktor syarat yang mempengaruhi nilai probabilitas, yang hampir mustahil untuk dianalisa satu persatu. Akibatnya, perhitungan tersebut menjadi sulit untuk dilakukan. Disinilah digunakan asumsi independensi yang sangat tinggi ( naif ), bahwa masing-masing petunjuk ( F1,F2 , ..., Fn ) saling bebas ( independen ) satu sama lain. Dengan asumsi tersebut, maka berlaku suatu kesamaan sebagai berikut: .. [NBC-06] Untuk i\u2260j , sehingga .. [NBC-07] Atau dapat dituliskan dalam notasi .. [NBC-08] yang dapat dijabarkan sebagai berikut .. [NBC-09] Persamaan di atas merupakan model dari teorema Na\u00efve Bayes yang selanjutnya akan digunakan dalam proses klasifikasi. Untuk klasifikasi dengan data kontinyu digunakan rumus Densitas Gauss : .. [NBC-10] Keterangan P : Peluang Xi : Atribut ke-i xi : Nilai Atribut ke-i Y : Kelas yang dicari yi : Sub-kelas yang dicari \u03bc : mean , menyatakan rata-rata dari seluruh atribut \u03c3 : Deviasi Standar, menyatakan varian dari seluruh atribut Contoh Program Naive bayes pada suatu data import pandas as pd df=pd.read_csv(\"data iris bunga tugas.csv\") df Output : sepallength sepalwidth petallength petalwidth class Class Prediksi 0 5.1 3.5 1.4 0.2 Iris-setosa NaN 1 4.9 3.0 1.4 0.2 Iris-setosa NaN 2 4.7 3.2 1.3 0.2 Iris-setosa NaN 3 4.6 3.1 1.5 0.2 Iris-setosa NaN 4 5.0 3.6 1.4 0.2 Iris-setosa NaN 5 5.4 3.9 1.7 0.4 Iris-setosa NaN 6 4.6 3.4 1.4 0.3 Iris-setosa NaN 7 5.0 3.4 1.5 0.2 Iris-setosa NaN 8 4.4 2.9 1.4 0.2 Iris-setosa NaN 9 4.9 3.1 1.5 0.1 Iris-setosa NaN 10 7.0 3.2 4.7 1.4 Iris-versicolor NaN 11 6.4 3.2 4.5 1.5 Iris-versicolor NaN 12 6.9 3.1 4.9 1.5 Iris-versicolor NaN 13 5.5 2.3 4.0 1.3 Iris-versicolor NaN 14 6.5 2.8 4.6 1.5 Iris-versicolor NaN 15 5.7 2.8 4.5 1.3 Iris-versicolor NaN 16 6.3 3.3 4.7 1.6 Iris-versicolor NaN 17 4.9 2.4 3.3 1.0 Iris-versicolor NaN 18 6.6 2.9 4.6 1.3 Iris-versicolor NaN 19 5.2 2.7 3.9 1.4 Iris-versicolor NaN 20 6.3 3.3 6.0 2.5 Iris-virginica NaN 21 5.8 2.7 5.1 1.9 Iris-virginica NaN 22 7.1 3.0 5.9 2.1 Iris-virginica NaN 23 6.3 2.9 5.6 1.8 Iris-virginica NaN 24 6.5 3.0 5.8 2.2 Iris-virginica NaN 25 7.6 3.0 6.6 2.1 Iris-virginica NaN 26 4.9 2.5 4.5 1.7 Iris-virginica NaN 27 7.3 2.9 6.3 1.8 Iris-virginica NaN 28 6.7 2.5 5.8 1.8 Iris-virginica NaN 29 7.2 3.6 6.1 2.5 Iris-virginica NaN Identifikasi grup class dataset_classes = {} # table per classes for key,group in dataset.groupby('class'): mu_s = [group[c].mean() for c in group.columns[:-1]] sigma_s = [group[c].std() for c in group.columns[:-1]] dataset_classes[key] = [group, mu_s, sigma_s] print(key, \"===>\") print('Mu_s =>', array(mu_s)) print('Sigma_s =>', array(sigma_s)) table(group) setosa ===> Mu_s => [5.18333333 3.56666667 1.51666667 0.31666667] Sigma_s => [0.3250641 0.22509257 0.14719601 0.1602082 ] sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) class 5 3.5 1.6 0.6 setosa 5.2 3.4 1.4 0.2 setosa 5 3.4 1.5 0.2 setosa 5.4 3.9 1.3 0.4 setosa 4.8 3.4 1.6 0.2 setosa 5.7 3.8 1.7 0.3 setosa versicolor ===> Mu_s => [5.89166667 2.76666667 4.13333333 1.26666667] Sigma_s => [0.52476546 0.33393884 0.46188022 0.21461735] sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) class 5.7 2.8 4.1 1.3 versicolor 5.5 2.6 4.4 1.2 versicolor 5.5 2.4 3.7 1 versicolor 6 3.4 4.5 1.6 versicolor 5.7 2.6 3.5 1 versicolor 6 2.9 4.5 1.5 versicolor 5.6 2.5 3.9 1.1 versicolor 6.8 2.8 4.8 1.4 versicolor 6.7 3.1 4.4 1.4 versicolor 5.8 2.6 4 1.2 versicolor 6.4 3.2 4.5 1.5 versicolor 5 2.3 3.3 1 versicolor virginica ===> Mu_s => [6.61666667 3.13333333 5.58333333 2.06666667] Sigma_s => [0.7790826 0.34465617 0.59670814 0.23868326] sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) class 6.3 3.3 6 2.5 virginica 6.4 3.1 5.5 1.8 virginica 5.8 2.7 5.1 1.9 virginica 6.7 3.1 5.6 2.4 virginica 6.5 3 5.2 2 virginica 5.6 2.8 4.9 2 virginica 5.9 3 5.1 1.8 virginica 7.7 3 6.1 2.3 virginica 6.8 3 5.5 2.1 virginica 7.7 3.8 6.7 2.2 virginica 6.1 3 4.9 1.8 virginica 7.9 3.8 6.4 2 virginica Probabilitas Prior dan Likehood def numericalPriorProbability(v, mu, sigma): return (1.0/sqrt(2 * pi * (sigma ** 2))*exp(-((v-mu)**2)/(2*(sigma**2)))) def categoricalProbability(sample,universe): return sample.shape[0]/universe.shape[0] Ps = ([[y]+[numericalPriorProbability(x, d[1][i], d[2][i]) for i,x in enumerate(test)]+ [categoricalProbability(d[0],dataset)] for y,d in dataset_classes.items()]) table(DataFrame(Ps, columns=[\"classes\"]+[\"P( %d | C )\" % d for d in test]+[\"P( C )\"])) classes P( 3 | C ) P( 5 | C ) P( 2 | C ) P( 4 | C ) P( C ) setosa 1.96232e-10 2.77721e-09 0.0123515 4.13093e-115 0.2 versicolor 1.93812e-07 2.31644e-10 2.01329e-05 1.11579e-35 0.4 virginica 1.07096e-05 4.94165e-07 9.87125e-09 9.46396e-15 0.4 Urutkan dengan perankingan Pss = ([[r[0], prod(r[1:])] for r in Ps]) PDss = DataFrame(Pss, columns=['class', 'probability']).sort_values('probability')[::-1] table(PDss) class probability virginica 1.97766e-34 versicolor 4.03412e-57 setosa 5.56132e-136 print(\"Prediksi Bayes untuk\", test, \"adalah\", PDss.values[0,0]) Prediksi Bayes untuk [3, 5, 2, 4] adalah virginica","title":"Tugas 4"},{"location":"tugas4/#tugas-4","text":"","title":"Tugas 4"},{"location":"tugas4/#naive-bayes-classifier","text":"Na\u00efve Bayes merupakan sebuah pengklasifikasian probabilistik sederhana yang menghitung sekumpulan probabilitas dengan menjumlahkan frekuensi dan kombinasi nilai dari dataset yang diberikan. Algoritma mengunakan teorema Bayes dan mengasumsikan semua atribut independen atau tidak saling ketergantungan yang diberikan oleh nilai pada variabel kelas ( Patil and Sherekar 2013 ). Definisi lain mengatakan Na\u00efve Bayes merupakan pengklasifikasian dengan metode probabilitas dan statistik yang dikemukan oleh ilmuwan Inggris Thomas Bayes, yaitu memprediksi peluang di masa depan berdasarkan pengalaman di masa sebelumnya ( Bustami 2013 ). Na\u00efve Bayes didasarkan pada asumsi penyederhanaan bahwa nilai atribut secara kondisional saling bebas jika diberikan nilai output. Dengan kata lain, diberikan nilai output, probabilitas mengamati secara bersama adalah produk dari probabilitas individu ( Ridwan 2013 ). Keuntungan penggunaan Naive Bayes adalah bahwa metode ini hanya membutuhkan jumlah data pelatihan (Training Data) yang kecil untuk menentukan estimasi paremeter yang diperlukan dalam proses pengklasifikasian. Naive Bayes sering bekerja jauh lebih baik dalam kebanyakan situasi dunia nyata yang kompleks dari pada yang diharapkan ( Pattekari and Parveen 2012 ) Naive Bayes Classifier dinilai bekerja sangat baik dibanding dengan model classifier lainnya, yaitu Na\u00efve Bayes Classifier memiliki tingkat akurasi yg lebih baik dibanding model classifier lainnya( Xhemali 2009 ).","title":"Naive Bayes Classifier"},{"location":"tugas4/#teorema-naive-bayes","text":"Sebelum menjelaskan Na\u00efve Bayes Classifier ini, akan dijelaskan terlebih dahulu Teorema Bayes yang menjadi dasar dari metoda tersebut. Pada Teorema Bayes , bila terdapat dua kejadian yang terpisah (misalkan X dan H ), maka Teorema Bayes dirumuskan sebagai berikut ( Bustami 2013 ).: .. [NBC-01] Keterangan Teorema Bayes sering pula dikembangkan mengingat berlakunya hukum probabilitas total, menjadi seperti berikut: .. [NBC-02] Keterangan Untuk menjelaskan Teorema Na\u00efve Bayes , perlu diketahui bahwa proses klasifikasi memerlukan sejumlah petunjuk untuk menentukan kelas apa yang cocok bagi sampel yang dianalisis tersebut. Karena itu, Teorema Bayes di atas disesuaikan sebagai berikut: .. [NBC-03] Di mana Variabel C merepresentasikan kelas, sementara variabel F1 ... Fn merepresentasikan karakteristik petunjuk yang dibutuhkan untuk melakukan klasifikasi. Maka rumus tersebut menjelaskan bahwa peluang masuknya sampel karakteristik tertentu dalam kelas C ( Posterior ) adalah peluang munculnya kelas C (sebelum masuknya sampel tersebut, seringkali disebut prior ), dikali dengan peluang kemunculan karakteristik-karakteristik sampel pada kelas C (disebut juga likelihood ), dibagi dengan peluang kemunculan karakteristik-karakteristik sampel secara global (disebut juga evidence ). Karena itu, rumus di atas dapat pula ditulis secara sederhana sebagai berikut: .. [NBC-04] Nilai Evidence selalu tetap untuk setiap kelas pada satu sampel . Nilai dari posterior tersebut nantinya akan dibandingkan dengan nilai-nilai posterior kelas lainnya untuk menentukan ke kelas apa suatu sampel akan diklasifikasikan. Penjabaran lebih lanjut rumus Bayes tersebut dilakukan dengan menjabarkan (C,F1, ... , Fn) menggunakan aturan perkalian sebagai berikut: .. [NBC-05] Dapat dilihat bahwa hasil penjabaran tersebut menyebabkan semakin banyak dan semakin kompleksnya faktor - faktor syarat yang mempengaruhi nilai probabilitas, yang hampir mustahil untuk dianalisa satu persatu. Akibatnya, perhitungan tersebut menjadi sulit untuk dilakukan. Disinilah digunakan asumsi independensi yang sangat tinggi ( naif ), bahwa masing-masing petunjuk ( F1,F2 , ..., Fn ) saling bebas ( independen ) satu sama lain. Dengan asumsi tersebut, maka berlaku suatu kesamaan sebagai berikut: .. [NBC-06] Untuk i\u2260j , sehingga .. [NBC-07] Atau dapat dituliskan dalam notasi .. [NBC-08] yang dapat dijabarkan sebagai berikut .. [NBC-09] Persamaan di atas merupakan model dari teorema Na\u00efve Bayes yang selanjutnya akan digunakan dalam proses klasifikasi. Untuk klasifikasi dengan data kontinyu digunakan rumus Densitas Gauss : .. [NBC-10] Keterangan P : Peluang Xi : Atribut ke-i xi : Nilai Atribut ke-i Y : Kelas yang dicari yi : Sub-kelas yang dicari \u03bc : mean , menyatakan rata-rata dari seluruh atribut \u03c3 : Deviasi Standar, menyatakan varian dari seluruh atribut","title":"Teorema Na\u00efve Bayes"},{"location":"tugas4/#contoh-program","text":"Naive bayes pada suatu data import pandas as pd df=pd.read_csv(\"data iris bunga tugas.csv\") df Output : sepallength sepalwidth petallength petalwidth class Class Prediksi 0 5.1 3.5 1.4 0.2 Iris-setosa NaN 1 4.9 3.0 1.4 0.2 Iris-setosa NaN 2 4.7 3.2 1.3 0.2 Iris-setosa NaN 3 4.6 3.1 1.5 0.2 Iris-setosa NaN 4 5.0 3.6 1.4 0.2 Iris-setosa NaN 5 5.4 3.9 1.7 0.4 Iris-setosa NaN 6 4.6 3.4 1.4 0.3 Iris-setosa NaN 7 5.0 3.4 1.5 0.2 Iris-setosa NaN 8 4.4 2.9 1.4 0.2 Iris-setosa NaN 9 4.9 3.1 1.5 0.1 Iris-setosa NaN 10 7.0 3.2 4.7 1.4 Iris-versicolor NaN 11 6.4 3.2 4.5 1.5 Iris-versicolor NaN 12 6.9 3.1 4.9 1.5 Iris-versicolor NaN 13 5.5 2.3 4.0 1.3 Iris-versicolor NaN 14 6.5 2.8 4.6 1.5 Iris-versicolor NaN 15 5.7 2.8 4.5 1.3 Iris-versicolor NaN 16 6.3 3.3 4.7 1.6 Iris-versicolor NaN 17 4.9 2.4 3.3 1.0 Iris-versicolor NaN 18 6.6 2.9 4.6 1.3 Iris-versicolor NaN 19 5.2 2.7 3.9 1.4 Iris-versicolor NaN 20 6.3 3.3 6.0 2.5 Iris-virginica NaN 21 5.8 2.7 5.1 1.9 Iris-virginica NaN 22 7.1 3.0 5.9 2.1 Iris-virginica NaN 23 6.3 2.9 5.6 1.8 Iris-virginica NaN 24 6.5 3.0 5.8 2.2 Iris-virginica NaN 25 7.6 3.0 6.6 2.1 Iris-virginica NaN 26 4.9 2.5 4.5 1.7 Iris-virginica NaN 27 7.3 2.9 6.3 1.8 Iris-virginica NaN 28 6.7 2.5 5.8 1.8 Iris-virginica NaN 29 7.2 3.6 6.1 2.5 Iris-virginica NaN Identifikasi grup class dataset_classes = {} # table per classes for key,group in dataset.groupby('class'): mu_s = [group[c].mean() for c in group.columns[:-1]] sigma_s = [group[c].std() for c in group.columns[:-1]] dataset_classes[key] = [group, mu_s, sigma_s] print(key, \"===>\") print('Mu_s =>', array(mu_s)) print('Sigma_s =>', array(sigma_s)) table(group) setosa ===> Mu_s => [5.18333333 3.56666667 1.51666667 0.31666667] Sigma_s => [0.3250641 0.22509257 0.14719601 0.1602082 ] sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) class 5 3.5 1.6 0.6 setosa 5.2 3.4 1.4 0.2 setosa 5 3.4 1.5 0.2 setosa 5.4 3.9 1.3 0.4 setosa 4.8 3.4 1.6 0.2 setosa 5.7 3.8 1.7 0.3 setosa versicolor ===> Mu_s => [5.89166667 2.76666667 4.13333333 1.26666667] Sigma_s => [0.52476546 0.33393884 0.46188022 0.21461735] sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) class 5.7 2.8 4.1 1.3 versicolor 5.5 2.6 4.4 1.2 versicolor 5.5 2.4 3.7 1 versicolor 6 3.4 4.5 1.6 versicolor 5.7 2.6 3.5 1 versicolor 6 2.9 4.5 1.5 versicolor 5.6 2.5 3.9 1.1 versicolor 6.8 2.8 4.8 1.4 versicolor 6.7 3.1 4.4 1.4 versicolor 5.8 2.6 4 1.2 versicolor 6.4 3.2 4.5 1.5 versicolor 5 2.3 3.3 1 versicolor virginica ===> Mu_s => [6.61666667 3.13333333 5.58333333 2.06666667] Sigma_s => [0.7790826 0.34465617 0.59670814 0.23868326] sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) class 6.3 3.3 6 2.5 virginica 6.4 3.1 5.5 1.8 virginica 5.8 2.7 5.1 1.9 virginica 6.7 3.1 5.6 2.4 virginica 6.5 3 5.2 2 virginica 5.6 2.8 4.9 2 virginica 5.9 3 5.1 1.8 virginica 7.7 3 6.1 2.3 virginica 6.8 3 5.5 2.1 virginica 7.7 3.8 6.7 2.2 virginica 6.1 3 4.9 1.8 virginica 7.9 3.8 6.4 2 virginica Probabilitas Prior dan Likehood def numericalPriorProbability(v, mu, sigma): return (1.0/sqrt(2 * pi * (sigma ** 2))*exp(-((v-mu)**2)/(2*(sigma**2)))) def categoricalProbability(sample,universe): return sample.shape[0]/universe.shape[0] Ps = ([[y]+[numericalPriorProbability(x, d[1][i], d[2][i]) for i,x in enumerate(test)]+ [categoricalProbability(d[0],dataset)] for y,d in dataset_classes.items()]) table(DataFrame(Ps, columns=[\"classes\"]+[\"P( %d | C )\" % d for d in test]+[\"P( C )\"])) classes P( 3 | C ) P( 5 | C ) P( 2 | C ) P( 4 | C ) P( C ) setosa 1.96232e-10 2.77721e-09 0.0123515 4.13093e-115 0.2 versicolor 1.93812e-07 2.31644e-10 2.01329e-05 1.11579e-35 0.4 virginica 1.07096e-05 4.94165e-07 9.87125e-09 9.46396e-15 0.4 Urutkan dengan perankingan Pss = ([[r[0], prod(r[1:])] for r in Ps]) PDss = DataFrame(Pss, columns=['class', 'probability']).sort_values('probability')[::-1] table(PDss) class probability virginica 1.97766e-34 versicolor 4.03412e-57 setosa 5.56132e-136 print(\"Prediksi Bayes untuk\", test, \"adalah\", PDss.values[0,0]) Prediksi Bayes untuk [3, 5, 2, 4] adalah virginica","title":"Contoh Program"},{"location":"tugas5/","text":"Tugas 5 Algoritma K-Nearest Neighborhood (K-NN) Pengertian KNN Algoritma K-Nearest Neighbor (K-NN) adalah sebuah metode klasifikasi terhadap sekumpulan data berdasarkan pembelajaran data yang sudah terklasifikasikan sebelumya. Termasuk dalam supervised learning, dimana hasil query instance yang baru diklasifikasikan berdasarkan mayoritas kedekatan jarak dari kategori yang ada dalam K-NN. Contoh kasus : Anda ingin mengambil sebuah keputusan (kelas) antara datang atau tidak datang ke sebuah pertemuan. Untuk mendukung pengambilan keputusan tersebut, Anda melihat mayoritas dari keputusan teman-teman Anda (instance lainnya). Teman-teman tersebut Anda pilih berdasarkan kedekatannya dengan Anda. Ukuran kedekatan pertemanan ini bisa bermacam-macam: tetangga, satu hobi, satu kelas, atau hal-hal lainnya. Ukuran-ukuran tersebut bisa juga digunakan bersamaan, misalnya si A itu tetangga, satu hobi, dan satu kelas; sedangkan si B hanya satu kelas saja. Dekat atau jauhnya tetangga biasanya dihitung berdasarkan Euclidean Distance atau Manhattan Distance. Kedekatan dapat dianggap sebagai invers jarak, alias berbanding terbalik dengan jarak. Semakin kecil jarak antara dua instance, semakin besar \u201ckedekatan\u201d antara dua instance tersebut. Dengan demikian, k nearest neighbours dari sebuah instance x didefinisikan sebagai k instance yang memiliki jarak terkecil (kedekatan terbesar, nearest) dengan x. Tahapan Langkah Algoritma K-NN Menentukan parameter k (jumlah tetangga paling dekat). Menghitung kuadrat jarak euclidean objek terhadap data training yang diberikan. Mengurutkan hasil no 2 secara ascending (berurutan dari nilai tinggi ke rendah) Mengumpulkan kategori Y (Klasifikasi nearest neighbor berdasarkan nilai k) Dengan menggunakan kategori nearest neighbor yang paling mayoritas maka dapat dipredisikan kategori objek. Contoh Problem Mencari 5 data terdekat menggunakan algoritma KNN. Tentukan data training sepal length(a) sepal width(b) petal length(c) petal width(d) variety(class) 4,9 3,1 1,5 0,1 Setosa 4,9 3 1,4 0,2 Setosa 4,8 3,1 1,6 0,2 Setosa 5 3 1,6 0,2 Setosa 4,8 3 1,4 0,1 Setosa 4,9 3,1 1,5 0,2 Setosa 4,8 3 1,4 0,3 Setosa 5 3,3 1,4 0,2 Setosa 4,7 3,2 1,6 0,2 Setosa 4,7 3,2 1,3 0,2 Setosa 4,6 3,1 1,5 0,2 Setosa 5 3,4 1,5 0,2 Setosa 5 3,2 1,2 0,2 Setosa 4,8 3,4 1,6 0,2 Setosa 4,6 3,2 1,4 0,2 Setosa 5,1 3,4 1,5 0,2 Setosa 5 3,4 1,6 0,4 Setosa 5,2 3,4 1,4 0,2 Setosa 4,6 3,4 1,4 0,3 Setosa 5,1 3,3 1,7 0,5 Setosa 5,1 3,5 1,4 0,2 Setosa 5,1 3,5 1,4 0,3 Setosa 5 3,5 1,3 0,3 Setosa 5,2 3,5 1,5 0,2 Setosa 4,8 3,4 1,9 0,2 Setosa 5 3,6 1,4 0,2 Setosa 4,9 3,6 1,4 0,1 Setosa 4,4 3 1,3 0,2 Setosa 4,4 3,2 1,3 0,2 Setosa 4,4 2,9 1,4 0,2 Setosa 5 3,5 1,6 0,6 Setosa 5,4 3,4 1,7 0,2 Setosa 5,4 3,4 1,5 0,4 Setosa 5,1 3,7 1,5 0,4 Setosa 5,3 3,7 1,5 0,2 Setosa 5,1 3,8 1,5 0,3 Setosa 5,1 3,8 1,6 0,2 Setosa 4,3 3 1,1 0,1 Setosa 5,5 3,5 1,3 0,2 Setosa 4,6 3,6 1 0,2 Setosa 5,4 3,7 1,5 0,2 Setosa 5,1 3,8 1,9 0,4 Setosa 4,5 2,3 1,3 0,3 Setosa 5,4 3,9 1,7 0,4 Setosa 5,4 3,9 1,3 0,4 Setosa 5,2 4,1 1,5 0,1 Setosa 5,7 3,8 1,7 0,3 Setosa 5,5 4,2 1,4 0,2 Setosa 5,8 4 1,2 0,2 Setosa 5,7 4,4 1,5 0,4 Setosa 5,1 2,5 3 1,1 Versicolor 5 2,3 3,3 1 Versicolor 5,7 2,6 3,5 1 Versicolor 5 2 3,5 1 Versicolor 5,6 2,9 3,6 1,3 Versicolor 5,5 2,4 3,7 1 Versicolor 5,5 2,4 3,8 1,1 Versicolor 5,6 2,5 3,9 1,1 Versicolor 5,2 2,7 3,9 1,4 Versicolor 5,8 2,7 3,9 1,2 Versicolor 5,5 2,5 4 1,3 Versicolor 5,8 2,6 4 1,2 Versicolor 5,8 2,7 4,1 1 Versicolor 5,5 2,3 4 1,3 Versicolor 5,6 3 4,1 1,3 Versicolor 5,7 2,8 4,1 1,3 Versicolor 6 2,2 4 1 Versicolor 5,7 3 4,2 1,2 Versicolor 6,1 2,8 4 1,3 Versicolor 5,6 2,7 4,2 1,3 Versicolor 5,7 2,9 4,2 1,3 Versicolor 5,9 3 4,2 1,5 Versicolor 5,5 2,6 4,4 1,2 Versicolor 6,2 2,9 4,3 1,3 Versicolor 5,7 2,8 4,5 1,3 Versicolor 5,4 3 4,5 1,5 Versicolor 5,6 3 4,5 1,5 Versicolor 6,4 2,9 4,3 1,3 Versicolor 4,9 2,5 4,5 1,7 Virginica 6 2,9 4,5 1,5 Versicolor 6,3 2,3 4,4 1,3 Versicolor 6 3,4 4,5 1,6 Versicolor 6,1 3 4,6 1,4 Versicolor 6,6 3 4,4 1,4 Versicolor 6,1 2,8 4,7 1,2 Versicolor 6,4 3,2 4,5 1,5 Versicolor 6,7 3,1 4,4 1,4 Versicolor 6,1 2,9 4,7 1,4 Versicolor 6,2 2,2 4,5 1,5 Versicolor 6,6 2,9 4,6 1,3 Versicolor 6,5 2,8 4,6 1,5 Versicolor 6,3 3,3 4,7 1,6 Versicolor 5,9 3,2 4,8 1,8 Versicolor 6 3 4,8 1,8 Virginica 6,7 3,1 4,7 1,5 Versicolor 6,2 2,8 4,8 1,8 Virginica 5,6 2,8 4,9 2 Virginica 6,3 2,5 4,9 1,5 Versicolor 6,1 3 4,9 1,8 Virginica 6 2,2 5 1,5 Virginica 6,8 2,8 4,8 1,4 Versicolor 7 3,2 4,7 1,4 Versicolor 6,3 2,7 4,9 1,8 Virginica 6 2,7 5,1 1,6 Versicolor 5,7 2,5 5 2 Virginica 5,9 3 5,1 1,8 Virginica 6,3 2,8 5,1 1,5 Virginica 5,8 2,7 5,1 1,9 Virginica 5,8 2,7 5,1 1,9 Virginica 6,9 3,1 4,9 1,5 Versicolor 6,3 2,5 5 1,9 Virginica 6,7 3 5 1,7 Versicolor 5,8 2,8 5,1 2,4 Virginica 6,5 3,2 5,1 2 Virginica 6,5 3 5,2 2 Virginica 6,4 2,7 5,3 1,9 Virginica 6,1 2,6 5,6 1,4 Virginica 6,4 3,1 5,5 1,8 Virginica 6,4 3,2 5,3 2,3 Virginica 6,5 3 5,5 1,8 Virginica 6,7 3 5,2 2,3 Virginica 6,3 2,9 5,6 1,8 Virginica 6,9 3,1 5,1 2,3 Virginica 6,2 3,4 5,4 2,3 Virginica 6,4 2,8 5,6 2,1 Virginica 6,9 3,1 5,4 2,1 Virginica 6,4 2,8 5,6 2,2 Virginica 6,8 3 5,5 2,1 Virginica 6,3 3,4 5,6 2,4 Virginica 6,7 3,3 5,7 2,1 Virginica 6,7 2,5 5,8 1,8 Virginica 6,7 3,1 5,6 2,4 Virginica 6,5 3 5,8 2,2 Virginica 7,2 3 5,8 1,6 Virginica 6,9 3,2 5,7 2,3 Virginica 6,7 3,3 5,7 2,5 Virginica 6,8 3,2 5,9 2,3 Virginica 6,3 3,3 6 2,5 Virginica 7,1 3 5,9 2,1 Virginica 7,2 3,2 6 1,8 Virginica 7,4 2,8 6,1 1,9 Virginica 7,3 2,9 6,3 1,8 Virginica 7,2 3,6 6,1 2,5 Virginica 7,7 3 6,1 2,3 Virginica 7,9 3,8 6,4 2 Virginica 7,6 3 6,6 2,1 Virginica 7,7 2,8 6,7 2 Virginica 7,7 3,8 6,7 2,2 Virginica 7,7 2,6 6,9 2,3 Virginica Tentukan sample data a b c d class 4,8 3,4 1,6 0,2 Setosa Menghitung jarak data $$ jarak = (data di tabel - std data sampel)^2 $$ (a-sd1)^2 (b-sd2)^2 (c-sd3)^2 (d-sd4)^2 SQRT 0,01 0,09 0,01 0,01 0,34641 0,01 0,16 0,04 0 0,458258 0 0,09 0 0 0,3 0,04 0,16 0 0 0,447214 0 0,16 0,04 0,01 0,458258 0,01 0,09 0,01 0 0,331662 0 0,16 0,04 0,01 0,458258 0,04 0,01 0,04 0 0,3 0,01 0,04 0 0 0,223607 0,01 0,04 0,09 0 0,374166 0,04 0,09 0,01 0 0,374166 0,04 0 0,01 0 0,223607 0,04 0,04 0,16 0 0,489898 0 0 0 0 0 0,04 0,04 0,04 0 0,34641 0,09 0 0,01 0 0,316228 0,04 0 0 0,04 0,282843 0,16 0 0,04 0 0,447214 0,04 0 0,04 0,01 0,3 0,09 0,01 0,01 0,09 0,447214 0,09 0,01 0,04 0 0,374166 0,09 0,01 0,04 0,01 0,387298 0,04 0,01 0,09 0,01 0,387298 0,16 0,01 0,01 0 0,424264 0 0 0,09 0 0,3 0,04 0,04 0,04 0 0,34641 0,01 0,04 0,04 0,01 0,316228 0,16 0,16 0,09 0 0,640312 0,16 0,04 0,09 0 0,538516 0,16 0,25 0,04 0 0,67082 0,04 0,01 0 0,16 0,458258 0,36 0 0,01 0 0,608276 0,36 0 0,01 0,04 0,640312 0,09 0,09 0,01 0,04 0,479583 0,25 0,09 0,01 0 0,591608 0,09 0,16 0,01 0,01 0,519615 0,09 0,16 0 0 0,5 0,25 0,16 0,25 0,01 0,818535 0,49 0,01 0,09 0 0,768115 0,04 0,04 0,36 0 0,663325 0,36 0,09 0,01 0 0,678233 0,09 0,16 0,09 0,04 0,616441 0,09 1,21 0,09 0,01 1,183216 0,36 0,25 0,01 0,04 0,812404 0,36 0,25 0,09 0,04 0,860233 0,16 0,49 0,01 0,01 0,818535 0,81 0,16 0,01 0,01 0,994987 0,49 0,64 0,04 0 1,081665 1 0,36 0,16 0 1,232883 0,81 1 0,01 0,04 1,363818 0,09 0,81 1,96 0,81 1,915724 0,04 1,21 2,89 0,64 2,186321 0,81 0,64 3,61 0,64 2,387467 0,04 1,96 3,61 0,64 2,5 0,64 0,25 4 1,21 2,469818 0,49 1 4,41 0,64 2,557342 0,49 1 4,84 0,81 2,672078 0,64 0,81 5,29 0,81 2,747726 0,16 0,49 5,29 1,44 2,716616 1 0,49 5,29 1 2,789265 0,49 0,81 5,76 1,21 2,875761 1 0,64 5,76 1 2,898275 1 0,49 6,25 0,64 2,894823 0,49 1,21 5,76 1,21 2,944486 0,64 0,16 6,25 1,21 2,874022 0,81 0,36 6,25 1,21 2,937686 1,44 1,44 5,76 0,64 3,046309 0,81 0,16 6,76 1 2,954657 1,69 0,36 5,76 1,21 3,003331 0,64 0,49 6,76 1,21 3,016621 0,81 0,25 6,76 1,21 3,004996 1,21 0,16 6,76 1,69 3,133688 0,49 0,64 7,84 1 3,157531 1,96 0,25 7,29 1,21 3,272614 0,81 0,36 8,41 1,21 3,284814 0,36 0,16 8,41 1,69 3,258834 0,64 0,16 8,41 1,69 3,301515 2,56 0,25 7,29 1,21 3,363034 0,01 0,81 8,41 2,25 3,388215 1,44 0,25 8,41 1,69 3,433657 2,25 1,21 7,84 1,21 3,536948 1,44 0 8,41 1,96 3,436568 1,69 0,16 9 1,44 3,50571 3,24 0,16 7,84 1,44 3,560899 1,69 0,36 9,61 1 3,558089 2,56 0,04 8,41 1,69 3,563706 3,61 0,09 7,84 1,44 3,602777 1,69 0,25 9,61 1,44 3,604164 1,96 1,44 8,41 1,69 3,674235 3,24 0,25 9 1,21 3,701351 2,89 0,36 9 1,69 3,733631 2,25 0,01 9,61 1,96 3,718871 1,21 0,04 10,24 2,56 3,748333 1,44 0,16 10,24 2,56 3,794733 3,61 0,09 9,61 1,69 3,872983 1,96 0,36 10,24 2,56 3,888444 0,64 0,36 10,89 3,24 3,88973 2,25 0,81 10,89 1,69 3,954744 1,69 0,16 10,89 2,56 3,911521 1,44 1,44 11,56 1,69 4,016217 4 0,36 10,24 1,44 4,004997 4,84 0,04 9,61 1,44 3,99124 2,25 0,49 10,89 2,56 4,02368 1,44 0,49 12,25 1,96 4,017462 0,81 0,81 11,56 3,24 4,05216 1,21 0,16 12,25 2,56 4,022437 2,25 0,36 12,25 1,69 4,068169 1 0,49 12,25 2,89 4,07799 1 0,49 12,25 2,89 4,07799 4,41 0,09 10,89 1,69 4,132796 2,25 0,81 11,56 2,89 4,184495 3,61 0,16 11,56 2,25 4,192851 1 0,36 12,25 4,84 4,295346 2,89 0,04 12,25 3,24 4,291853 2,89 0,16 12,96 3,24 4,387482 2,56 0,49 13,69 2,89 4,430576 1,69 0,64 16 1,44 4,446347 2,56 0,09 15,21 2,56 4,518849 2,56 0,04 13,69 4,41 4,549725 2,89 0,16 15,21 2,56 4,562894 3,61 0,16 12,96 4,41 4,597826 2,25 0,25 16 2,56 4,589118 4,41 0,09 12,25 4,41 4,6 1,96 0 14,44 4,41 4,561798 2,56 0,36 16 3,61 4,746578 4,41 0,09 14,44 3,61 4,748684 2,56 0,36 16 4 4,787484 4 0,16 15,21 3,61 4,793746 2,25 0 16 4,84 4,805206 3,61 0,01 16,81 3,61 4,90306 3,61 0,81 17,64 2,56 4,961854 3,61 0,09 16 4,84 4,953786 2,89 0,16 17,64 4 4,968903 5,76 0,16 17,64 1,96 5,051732 4,41 0,04 16,81 4,41 5,066557 3,61 0,01 16,81 5,29 5,071489 4 0,04 18,49 4,41 5,190376 2,25 0,01 19,36 5,29 5,187485 5,29 0,16 18,49 3,61 5,248809 5,76 0,04 19,36 2,56 5,264979 6,76 0,36 20,25 2,89 5,500909 6,25 0,25 22,09 2,56 5,581219 5,76 0,04 20,25 5,29 5,598214 8,41 0,16 20,25 4,41 5,764547 9,61 0,16 23,04 3,24 6,004165 7,84 0,16 25 3,61 6,05062 8,41 0,36 26,01 3,24 6,166036 8,41 0,16 26,01 4 6,21128 8,41 0,64 28,09 4,41 6,445929 Cari 5 data dengan jarak paling pendek (pada kolom SQRT) dari data training atau dengan melakukan Sorting data dari hasil SQRT. a b c d class Jarak 4,7 3,2 1,6 0,2 Setosa 0,223607 5 3,4 1,5 0,2 Setosa 0,223607 5 3,4 1,6 0,4 Setosa 0,282843 4,8 3,1 1,6 0,2 Setosa 0,3 4,6 3,4 1,4 0,3 Setosa 0,3 Hitung berat antar variasi dengan rumus : $$ variasi = 1/jarak $$ jarak variasi versicolor setosa virginica 0,223607 4,47213191 0 0,223607 0 0,223607 4,47213191 0 0,223607 0 0,282843 3,53553031 0 0,282843 0 0,3 3,33333333 0 0,3 0 0,3 3,33333333 0 0,3 0 Kesimpulan : Nilai terbesar dari ketiga class tersebut adalah SETOSA . . . . . Referensi : https://informatikalogi.com/algoritma-k-nn-k-nearest-neighbor/","title":"Tugas 5"},{"location":"tugas5/#tugas-5","text":"","title":"Tugas 5"},{"location":"tugas5/#algoritma-k-nearest-neighborhood-k-nn","text":"","title":"Algoritma K-Nearest Neighborhood (K-NN)"},{"location":"tugas5/#pengertian-knn","text":"Algoritma K-Nearest Neighbor (K-NN) adalah sebuah metode klasifikasi terhadap sekumpulan data berdasarkan pembelajaran data yang sudah terklasifikasikan sebelumya. Termasuk dalam supervised learning, dimana hasil query instance yang baru diklasifikasikan berdasarkan mayoritas kedekatan jarak dari kategori yang ada dalam K-NN. Contoh kasus : Anda ingin mengambil sebuah keputusan (kelas) antara datang atau tidak datang ke sebuah pertemuan. Untuk mendukung pengambilan keputusan tersebut, Anda melihat mayoritas dari keputusan teman-teman Anda (instance lainnya). Teman-teman tersebut Anda pilih berdasarkan kedekatannya dengan Anda. Ukuran kedekatan pertemanan ini bisa bermacam-macam: tetangga, satu hobi, satu kelas, atau hal-hal lainnya. Ukuran-ukuran tersebut bisa juga digunakan bersamaan, misalnya si A itu tetangga, satu hobi, dan satu kelas; sedangkan si B hanya satu kelas saja. Dekat atau jauhnya tetangga biasanya dihitung berdasarkan Euclidean Distance atau Manhattan Distance. Kedekatan dapat dianggap sebagai invers jarak, alias berbanding terbalik dengan jarak. Semakin kecil jarak antara dua instance, semakin besar \u201ckedekatan\u201d antara dua instance tersebut. Dengan demikian, k nearest neighbours dari sebuah instance x didefinisikan sebagai k instance yang memiliki jarak terkecil (kedekatan terbesar, nearest) dengan x.","title":"Pengertian KNN"},{"location":"tugas5/#tahapan-langkah-algoritma-k-nn","text":"Menentukan parameter k (jumlah tetangga paling dekat). Menghitung kuadrat jarak euclidean objek terhadap data training yang diberikan. Mengurutkan hasil no 2 secara ascending (berurutan dari nilai tinggi ke rendah) Mengumpulkan kategori Y (Klasifikasi nearest neighbor berdasarkan nilai k) Dengan menggunakan kategori nearest neighbor yang paling mayoritas maka dapat dipredisikan kategori objek.","title":"Tahapan Langkah Algoritma K-NN"},{"location":"tugas5/#contoh-problem","text":"","title":"Contoh Problem"},{"location":"tugas5/#mencari-5-data-terdekat-menggunakan-algoritma-knn","text":"","title":"Mencari 5 data terdekat menggunakan algoritma KNN."},{"location":"tugas5/#tentukan-data-training","text":"sepal length(a) sepal width(b) petal length(c) petal width(d) variety(class) 4,9 3,1 1,5 0,1 Setosa 4,9 3 1,4 0,2 Setosa 4,8 3,1 1,6 0,2 Setosa 5 3 1,6 0,2 Setosa 4,8 3 1,4 0,1 Setosa 4,9 3,1 1,5 0,2 Setosa 4,8 3 1,4 0,3 Setosa 5 3,3 1,4 0,2 Setosa 4,7 3,2 1,6 0,2 Setosa 4,7 3,2 1,3 0,2 Setosa 4,6 3,1 1,5 0,2 Setosa 5 3,4 1,5 0,2 Setosa 5 3,2 1,2 0,2 Setosa 4,8 3,4 1,6 0,2 Setosa 4,6 3,2 1,4 0,2 Setosa 5,1 3,4 1,5 0,2 Setosa 5 3,4 1,6 0,4 Setosa 5,2 3,4 1,4 0,2 Setosa 4,6 3,4 1,4 0,3 Setosa 5,1 3,3 1,7 0,5 Setosa 5,1 3,5 1,4 0,2 Setosa 5,1 3,5 1,4 0,3 Setosa 5 3,5 1,3 0,3 Setosa 5,2 3,5 1,5 0,2 Setosa 4,8 3,4 1,9 0,2 Setosa 5 3,6 1,4 0,2 Setosa 4,9 3,6 1,4 0,1 Setosa 4,4 3 1,3 0,2 Setosa 4,4 3,2 1,3 0,2 Setosa 4,4 2,9 1,4 0,2 Setosa 5 3,5 1,6 0,6 Setosa 5,4 3,4 1,7 0,2 Setosa 5,4 3,4 1,5 0,4 Setosa 5,1 3,7 1,5 0,4 Setosa 5,3 3,7 1,5 0,2 Setosa 5,1 3,8 1,5 0,3 Setosa 5,1 3,8 1,6 0,2 Setosa 4,3 3 1,1 0,1 Setosa 5,5 3,5 1,3 0,2 Setosa 4,6 3,6 1 0,2 Setosa 5,4 3,7 1,5 0,2 Setosa 5,1 3,8 1,9 0,4 Setosa 4,5 2,3 1,3 0,3 Setosa 5,4 3,9 1,7 0,4 Setosa 5,4 3,9 1,3 0,4 Setosa 5,2 4,1 1,5 0,1 Setosa 5,7 3,8 1,7 0,3 Setosa 5,5 4,2 1,4 0,2 Setosa 5,8 4 1,2 0,2 Setosa 5,7 4,4 1,5 0,4 Setosa 5,1 2,5 3 1,1 Versicolor 5 2,3 3,3 1 Versicolor 5,7 2,6 3,5 1 Versicolor 5 2 3,5 1 Versicolor 5,6 2,9 3,6 1,3 Versicolor 5,5 2,4 3,7 1 Versicolor 5,5 2,4 3,8 1,1 Versicolor 5,6 2,5 3,9 1,1 Versicolor 5,2 2,7 3,9 1,4 Versicolor 5,8 2,7 3,9 1,2 Versicolor 5,5 2,5 4 1,3 Versicolor 5,8 2,6 4 1,2 Versicolor 5,8 2,7 4,1 1 Versicolor 5,5 2,3 4 1,3 Versicolor 5,6 3 4,1 1,3 Versicolor 5,7 2,8 4,1 1,3 Versicolor 6 2,2 4 1 Versicolor 5,7 3 4,2 1,2 Versicolor 6,1 2,8 4 1,3 Versicolor 5,6 2,7 4,2 1,3 Versicolor 5,7 2,9 4,2 1,3 Versicolor 5,9 3 4,2 1,5 Versicolor 5,5 2,6 4,4 1,2 Versicolor 6,2 2,9 4,3 1,3 Versicolor 5,7 2,8 4,5 1,3 Versicolor 5,4 3 4,5 1,5 Versicolor 5,6 3 4,5 1,5 Versicolor 6,4 2,9 4,3 1,3 Versicolor 4,9 2,5 4,5 1,7 Virginica 6 2,9 4,5 1,5 Versicolor 6,3 2,3 4,4 1,3 Versicolor 6 3,4 4,5 1,6 Versicolor 6,1 3 4,6 1,4 Versicolor 6,6 3 4,4 1,4 Versicolor 6,1 2,8 4,7 1,2 Versicolor 6,4 3,2 4,5 1,5 Versicolor 6,7 3,1 4,4 1,4 Versicolor 6,1 2,9 4,7 1,4 Versicolor 6,2 2,2 4,5 1,5 Versicolor 6,6 2,9 4,6 1,3 Versicolor 6,5 2,8 4,6 1,5 Versicolor 6,3 3,3 4,7 1,6 Versicolor 5,9 3,2 4,8 1,8 Versicolor 6 3 4,8 1,8 Virginica 6,7 3,1 4,7 1,5 Versicolor 6,2 2,8 4,8 1,8 Virginica 5,6 2,8 4,9 2 Virginica 6,3 2,5 4,9 1,5 Versicolor 6,1 3 4,9 1,8 Virginica 6 2,2 5 1,5 Virginica 6,8 2,8 4,8 1,4 Versicolor 7 3,2 4,7 1,4 Versicolor 6,3 2,7 4,9 1,8 Virginica 6 2,7 5,1 1,6 Versicolor 5,7 2,5 5 2 Virginica 5,9 3 5,1 1,8 Virginica 6,3 2,8 5,1 1,5 Virginica 5,8 2,7 5,1 1,9 Virginica 5,8 2,7 5,1 1,9 Virginica 6,9 3,1 4,9 1,5 Versicolor 6,3 2,5 5 1,9 Virginica 6,7 3 5 1,7 Versicolor 5,8 2,8 5,1 2,4 Virginica 6,5 3,2 5,1 2 Virginica 6,5 3 5,2 2 Virginica 6,4 2,7 5,3 1,9 Virginica 6,1 2,6 5,6 1,4 Virginica 6,4 3,1 5,5 1,8 Virginica 6,4 3,2 5,3 2,3 Virginica 6,5 3 5,5 1,8 Virginica 6,7 3 5,2 2,3 Virginica 6,3 2,9 5,6 1,8 Virginica 6,9 3,1 5,1 2,3 Virginica 6,2 3,4 5,4 2,3 Virginica 6,4 2,8 5,6 2,1 Virginica 6,9 3,1 5,4 2,1 Virginica 6,4 2,8 5,6 2,2 Virginica 6,8 3 5,5 2,1 Virginica 6,3 3,4 5,6 2,4 Virginica 6,7 3,3 5,7 2,1 Virginica 6,7 2,5 5,8 1,8 Virginica 6,7 3,1 5,6 2,4 Virginica 6,5 3 5,8 2,2 Virginica 7,2 3 5,8 1,6 Virginica 6,9 3,2 5,7 2,3 Virginica 6,7 3,3 5,7 2,5 Virginica 6,8 3,2 5,9 2,3 Virginica 6,3 3,3 6 2,5 Virginica 7,1 3 5,9 2,1 Virginica 7,2 3,2 6 1,8 Virginica 7,4 2,8 6,1 1,9 Virginica 7,3 2,9 6,3 1,8 Virginica 7,2 3,6 6,1 2,5 Virginica 7,7 3 6,1 2,3 Virginica 7,9 3,8 6,4 2 Virginica 7,6 3 6,6 2,1 Virginica 7,7 2,8 6,7 2 Virginica 7,7 3,8 6,7 2,2 Virginica 7,7 2,6 6,9 2,3 Virginica","title":"Tentukan data training"},{"location":"tugas5/#tentukan-sample-data","text":"a b c d class 4,8 3,4 1,6 0,2 Setosa","title":"Tentukan sample data"},{"location":"tugas5/#menghitung-jarak-data","text":"$$ jarak = (data di tabel - std data sampel)^2 $$ (a-sd1)^2 (b-sd2)^2 (c-sd3)^2 (d-sd4)^2 SQRT 0,01 0,09 0,01 0,01 0,34641 0,01 0,16 0,04 0 0,458258 0 0,09 0 0 0,3 0,04 0,16 0 0 0,447214 0 0,16 0,04 0,01 0,458258 0,01 0,09 0,01 0 0,331662 0 0,16 0,04 0,01 0,458258 0,04 0,01 0,04 0 0,3 0,01 0,04 0 0 0,223607 0,01 0,04 0,09 0 0,374166 0,04 0,09 0,01 0 0,374166 0,04 0 0,01 0 0,223607 0,04 0,04 0,16 0 0,489898 0 0 0 0 0 0,04 0,04 0,04 0 0,34641 0,09 0 0,01 0 0,316228 0,04 0 0 0,04 0,282843 0,16 0 0,04 0 0,447214 0,04 0 0,04 0,01 0,3 0,09 0,01 0,01 0,09 0,447214 0,09 0,01 0,04 0 0,374166 0,09 0,01 0,04 0,01 0,387298 0,04 0,01 0,09 0,01 0,387298 0,16 0,01 0,01 0 0,424264 0 0 0,09 0 0,3 0,04 0,04 0,04 0 0,34641 0,01 0,04 0,04 0,01 0,316228 0,16 0,16 0,09 0 0,640312 0,16 0,04 0,09 0 0,538516 0,16 0,25 0,04 0 0,67082 0,04 0,01 0 0,16 0,458258 0,36 0 0,01 0 0,608276 0,36 0 0,01 0,04 0,640312 0,09 0,09 0,01 0,04 0,479583 0,25 0,09 0,01 0 0,591608 0,09 0,16 0,01 0,01 0,519615 0,09 0,16 0 0 0,5 0,25 0,16 0,25 0,01 0,818535 0,49 0,01 0,09 0 0,768115 0,04 0,04 0,36 0 0,663325 0,36 0,09 0,01 0 0,678233 0,09 0,16 0,09 0,04 0,616441 0,09 1,21 0,09 0,01 1,183216 0,36 0,25 0,01 0,04 0,812404 0,36 0,25 0,09 0,04 0,860233 0,16 0,49 0,01 0,01 0,818535 0,81 0,16 0,01 0,01 0,994987 0,49 0,64 0,04 0 1,081665 1 0,36 0,16 0 1,232883 0,81 1 0,01 0,04 1,363818 0,09 0,81 1,96 0,81 1,915724 0,04 1,21 2,89 0,64 2,186321 0,81 0,64 3,61 0,64 2,387467 0,04 1,96 3,61 0,64 2,5 0,64 0,25 4 1,21 2,469818 0,49 1 4,41 0,64 2,557342 0,49 1 4,84 0,81 2,672078 0,64 0,81 5,29 0,81 2,747726 0,16 0,49 5,29 1,44 2,716616 1 0,49 5,29 1 2,789265 0,49 0,81 5,76 1,21 2,875761 1 0,64 5,76 1 2,898275 1 0,49 6,25 0,64 2,894823 0,49 1,21 5,76 1,21 2,944486 0,64 0,16 6,25 1,21 2,874022 0,81 0,36 6,25 1,21 2,937686 1,44 1,44 5,76 0,64 3,046309 0,81 0,16 6,76 1 2,954657 1,69 0,36 5,76 1,21 3,003331 0,64 0,49 6,76 1,21 3,016621 0,81 0,25 6,76 1,21 3,004996 1,21 0,16 6,76 1,69 3,133688 0,49 0,64 7,84 1 3,157531 1,96 0,25 7,29 1,21 3,272614 0,81 0,36 8,41 1,21 3,284814 0,36 0,16 8,41 1,69 3,258834 0,64 0,16 8,41 1,69 3,301515 2,56 0,25 7,29 1,21 3,363034 0,01 0,81 8,41 2,25 3,388215 1,44 0,25 8,41 1,69 3,433657 2,25 1,21 7,84 1,21 3,536948 1,44 0 8,41 1,96 3,436568 1,69 0,16 9 1,44 3,50571 3,24 0,16 7,84 1,44 3,560899 1,69 0,36 9,61 1 3,558089 2,56 0,04 8,41 1,69 3,563706 3,61 0,09 7,84 1,44 3,602777 1,69 0,25 9,61 1,44 3,604164 1,96 1,44 8,41 1,69 3,674235 3,24 0,25 9 1,21 3,701351 2,89 0,36 9 1,69 3,733631 2,25 0,01 9,61 1,96 3,718871 1,21 0,04 10,24 2,56 3,748333 1,44 0,16 10,24 2,56 3,794733 3,61 0,09 9,61 1,69 3,872983 1,96 0,36 10,24 2,56 3,888444 0,64 0,36 10,89 3,24 3,88973 2,25 0,81 10,89 1,69 3,954744 1,69 0,16 10,89 2,56 3,911521 1,44 1,44 11,56 1,69 4,016217 4 0,36 10,24 1,44 4,004997 4,84 0,04 9,61 1,44 3,99124 2,25 0,49 10,89 2,56 4,02368 1,44 0,49 12,25 1,96 4,017462 0,81 0,81 11,56 3,24 4,05216 1,21 0,16 12,25 2,56 4,022437 2,25 0,36 12,25 1,69 4,068169 1 0,49 12,25 2,89 4,07799 1 0,49 12,25 2,89 4,07799 4,41 0,09 10,89 1,69 4,132796 2,25 0,81 11,56 2,89 4,184495 3,61 0,16 11,56 2,25 4,192851 1 0,36 12,25 4,84 4,295346 2,89 0,04 12,25 3,24 4,291853 2,89 0,16 12,96 3,24 4,387482 2,56 0,49 13,69 2,89 4,430576 1,69 0,64 16 1,44 4,446347 2,56 0,09 15,21 2,56 4,518849 2,56 0,04 13,69 4,41 4,549725 2,89 0,16 15,21 2,56 4,562894 3,61 0,16 12,96 4,41 4,597826 2,25 0,25 16 2,56 4,589118 4,41 0,09 12,25 4,41 4,6 1,96 0 14,44 4,41 4,561798 2,56 0,36 16 3,61 4,746578 4,41 0,09 14,44 3,61 4,748684 2,56 0,36 16 4 4,787484 4 0,16 15,21 3,61 4,793746 2,25 0 16 4,84 4,805206 3,61 0,01 16,81 3,61 4,90306 3,61 0,81 17,64 2,56 4,961854 3,61 0,09 16 4,84 4,953786 2,89 0,16 17,64 4 4,968903 5,76 0,16 17,64 1,96 5,051732 4,41 0,04 16,81 4,41 5,066557 3,61 0,01 16,81 5,29 5,071489 4 0,04 18,49 4,41 5,190376 2,25 0,01 19,36 5,29 5,187485 5,29 0,16 18,49 3,61 5,248809 5,76 0,04 19,36 2,56 5,264979 6,76 0,36 20,25 2,89 5,500909 6,25 0,25 22,09 2,56 5,581219 5,76 0,04 20,25 5,29 5,598214 8,41 0,16 20,25 4,41 5,764547 9,61 0,16 23,04 3,24 6,004165 7,84 0,16 25 3,61 6,05062 8,41 0,36 26,01 3,24 6,166036 8,41 0,16 26,01 4 6,21128 8,41 0,64 28,09 4,41 6,445929","title":"Menghitung jarak data"},{"location":"tugas5/#cari-5-data-dengan-jarak-paling-pendek-pada-kolom-sqrt-dari-data-training-atau-dengan-melakukan-sorting-data-dari-hasil-sqrt","text":"a b c d class Jarak 4,7 3,2 1,6 0,2 Setosa 0,223607 5 3,4 1,5 0,2 Setosa 0,223607 5 3,4 1,6 0,4 Setosa 0,282843 4,8 3,1 1,6 0,2 Setosa 0,3 4,6 3,4 1,4 0,3 Setosa 0,3","title":"Cari 5 data dengan jarak paling pendek (pada kolom SQRT) dari data training atau dengan melakukan Sorting data dari hasil SQRT."},{"location":"tugas5/#hitung-berat-antar-variasi-dengan-rumus","text":"$$ variasi = 1/jarak $$ jarak variasi versicolor setosa virginica 0,223607 4,47213191 0 0,223607 0 0,223607 4,47213191 0 0,223607 0 0,282843 3,53553031 0 0,282843 0 0,3 3,33333333 0 0,3 0 0,3 3,33333333 0 0,3 0","title":"Hitung berat antar variasi dengan rumus :"},{"location":"tugas5/#kesimpulan","text":"Nilai terbesar dari ketiga class tersebut adalah SETOSA . . . . . Referensi : https://informatikalogi.com/algoritma-k-nn-k-nearest-neighbor/","title":"Kesimpulan :"}]}